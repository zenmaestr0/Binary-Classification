{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import preprocessing\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\nfrom numpy import savetxt\nfrom scipy import stats\nfrom keras import Sequential\nfrom keras.layers.core import Dense, Dropout\nimport tensorflow as tf\nfrom keras.wrappers.scikit_learn import KerasClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-28T09:02:33.443881Z","iopub.execute_input":"2021-06-28T09:02:33.444342Z","iopub.status.idle":"2021-06-28T09:02:35.864480Z","shell.execute_reply.started":"2021-06-28T09:02:33.444219Z","shell.execute_reply":"2021-06-28T09:02:35.863330Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## **1. Prétraitement de données**","metadata":{}},{"cell_type":"markdown","source":"#### **1.1. Prétraitement de données d'apprentissage**","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/les-variables-des-images-de-publicit/data_train.csv\", sep='\\t') #dataframe object\nprint(df_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:35.866937Z","iopub.execute_input":"2021-06-28T09:02:35.867334Z","iopub.status.idle":"2021-06-28T09:02:36.442745Z","shell.execute_reply.started":"2021-06-28T09:02:35.867292Z","shell.execute_reply":"2021-06-28T09:02:36.441758Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(2459, 1559)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **1.1.1. Suppression des doublons**","metadata":{}},{"cell_type":"code","source":"# Sélectionnez des rangées en double, sauf la première occurrence, en fonction de toutes les colonnes :\ndf_duplicateRows = df_train[df_train.duplicated()]\nprint(\"Les rangées en double:\", df_duplicateRows)\nprint(df_duplicateRows.shape[0]) # 562 rangées  / 2459 rangées \n\n# Suppression des rangées en double:\ndf_train = df_train.drop_duplicates() # 2459 - 562 = 1897 rangées \nprint(df_train.shape) # (1897, 1559)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:36.444323Z","iopub.execute_input":"2021-06-28T09:02:36.444584Z","iopub.status.idle":"2021-06-28T09:02:36.926469Z","shell.execute_reply.started":"2021-06-28T09:02:36.444559Z","shell.execute_reply":"2021-06-28T09:02:36.925387Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Les rangées en double:          X1     X2      X3   X4  X5  X6  X7  X8  X9  X10  ...  X1550  X1551  \\\n12     33.0  230.0  6.9696  1.0   0   0   0   0   0    0  ...      0      0   \n31     50.0  107.0  2.1400  0.0   0   0   0   0   0    0  ...      0      0   \n34     60.0  468.0  7.8000  0.0   0   0   0   0   0    0  ...      0      0   \n49      NaN    NaN     NaN  0.0   0   0   0   0   0    0  ...      0      0   \n50      NaN    NaN     NaN  0.0   0   0   0   0   0    0  ...      0      0   \n...     ...    ...     ...  ...  ..  ..  ..  ..  ..  ...  ...    ...    ...   \n2448  134.0  184.0  1.3731  0.0   0   0   0   0   0    0  ...      0      0   \n2451   25.0  100.0  4.0000  1.0   0   0   0   0   0    0  ...      0      0   \n2452    NaN    NaN     NaN  1.0   0   0   0   0   0    0  ...      0      0   \n2453    NaN    NaN     NaN  1.0   0   0   0   0   0    0  ...      0      0   \n2458    NaN    NaN     NaN  1.0   0   0   0   0   0    0  ...      0      0   \n\n      X1552  X1553  X1554  X1555  X1556  X1557  X1558  outcome  \n12        0      0      0      0      0      0      0      ad.  \n31        0      0      0      0      0      0      0      ad.  \n34        0      0      0      0      0      0      0      ad.  \n49        0      0      0      0      0      0      0      ad.  \n50        0      0      0      0      0      0      0      ad.  \n...     ...    ...    ...    ...    ...    ...    ...      ...  \n2448      0      0      0      0      0      0      0   nonad.  \n2451      0      0      0      0      0      0      0   nonad.  \n2452      0      0      0      0      0      0      0   nonad.  \n2453      0      0      0      0      0      0      0   nonad.  \n2458      0      0      0      0      0      0      0   nonad.  \n\n[562 rows x 1559 columns]\n562\n(1897, 1559)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **1.1.2. Remplissage des valeurs manquantes**","metadata":{}},{"cell_type":"markdown","source":"#### Ratio des valeurs manquantes:","metadata":{}},{"cell_type":"code","source":"# le nombre de points de données manquants:\nmissing_values_count = df_train.isna().sum()\nprint(\"missing values count:\\n\", missing_values_count)\n\ntotal_cells = np.product(df_train.shape)\ntotal_missing = missing_values_count.sum()\n\nprint(\"total_cells:\", total_cells)\nprint(\"total_missing:\",total_missing)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:36.927661Z","iopub.execute_input":"2021-06-28T09:02:36.927984Z","iopub.status.idle":"2021-06-28T09:02:36.947539Z","shell.execute_reply.started":"2021-06-28T09:02:36.927950Z","shell.execute_reply":"2021-06-28T09:02:36.946784Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"missing values count:\n X1         386\nX2         382\nX3         390\nX4          10\nX5           0\n          ... \nX1555        0\nX1556        0\nX1557        0\nX1558        0\noutcome      0\nLength: 1559, dtype: int64\ntotal_cells: 2957423\ntotal_missing: 1168\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train['X1'].fillna(df_train['X1'].median(), inplace=True)\ndf_train['X2'].fillna(df_train['X2'].median(), inplace=True)\ndf_train['X3'].fillna(df_train['X3'].median(), inplace=True)\ndf_train['X4'].fillna(df_train['X4'].median(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:36.948655Z","iopub.execute_input":"2021-06-28T09:02:36.948941Z","iopub.status.idle":"2021-06-28T09:02:36.957727Z","shell.execute_reply.started":"2021-06-28T09:02:36.948913Z","shell.execute_reply":"2021-06-28T09:02:36.957017Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### **1.2. Prétraitement de données expérimentales**","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/les-variables-des-images-de-publicit/data_test.csv\", sep='\\t') \nprint(df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:36.958533Z","iopub.execute_input":"2021-06-28T09:02:36.958759Z","iopub.status.idle":"2021-06-28T09:02:37.241462Z","shell.execute_reply.started":"2021-06-28T09:02:36.958737Z","shell.execute_reply":"2021-06-28T09:02:37.240534Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(820, 1558)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **1.2.1. Remplissage des valeurs manquantes**","metadata":{}},{"cell_type":"code","source":"df_nans = df_test[df_test.isnull().any(axis=1)] # 394 rangées ont une ou plusieurs valeurs NaN\n# print(df_nans)\n\ndf_test['X1'].fillna(df_test['X1'].median(), inplace=True)\ndf_test['X2'].fillna(df_test['X2'].median(), inplace=True)\ndf_test['X3'].fillna(df_test['X3'].median(), inplace=True)\ndf_test['X4'].fillna(df_test['X4'].median(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:37.242604Z","iopub.execute_input":"2021-06-28T09:02:37.242878Z","iopub.status.idle":"2021-06-28T09:02:37.259537Z","shell.execute_reply.started":"2021-06-28T09:02:37.242852Z","shell.execute_reply":"2021-06-28T09:02:37.258771Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### **1. 3. La normalisation de données d'apprentissage et expérimentales**","metadata":{}},{"cell_type":"markdown","source":"#### **1.3.1. Séparation des dataframes numériques et booléennes**","metadata":{}},{"cell_type":"markdown","source":"#### Création de deux dataframes, car la première partie est constituée de variables continues (flottantes) et la seconde partie de variables booléennes :","metadata":{}},{"cell_type":"code","source":"df_train_fl = df_train.iloc[: , :4]\n# print(\"df_pubData_Train_fl:\\n\", df_train_fl.head())\n\ndf_train_bool = df_train.iloc[: , 4:]\n# print(\"boolean dataframe:\\n\", df_train_bool.head())","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:37.263061Z","iopub.execute_input":"2021-06-28T09:02:37.263324Z","iopub.status.idle":"2021-06-28T09:02:37.279673Z","shell.execute_reply.started":"2021-06-28T09:02:37.263289Z","shell.execute_reply":"2021-06-28T09:02:37.278675Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### Même division des bases de données pour les données expérimentales:","metadata":{}},{"cell_type":"code","source":"df_test_fl = df_test.iloc[: , :4]\n# print(\"df_pubData_Train_fl:\\n\", df_test_fl.head())\n\ndf_test_bool = df_test.iloc[: , 4:]\n# print(\"boolean dataframe:\\n\",df_test_bool.head())\nprint(df_test_bool.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:37.281882Z","iopub.execute_input":"2021-06-28T09:02:37.282121Z","iopub.status.idle":"2021-06-28T09:02:37.298837Z","shell.execute_reply.started":"2021-06-28T09:02:37.282097Z","shell.execute_reply":"2021-06-28T09:02:37.297820Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(820, 1554)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **1. 3. 2. Visualisation et suppression des valeurs aberrantes de la base de données d'apprentissage numérique** ","metadata":{}},{"cell_type":"markdown","source":"#### En utilisant le graphique ci-dessous, nous pouvons visualiser les valeurs aberrantes des colonnes numériques comme points individuels.","metadata":{}},{"cell_type":"code","source":"X1_column = df_train_fl[\"X1\"]\nX2_column = df_train_fl[\"X2\"]\nX3_column = df_train_fl[\"X3\"]\n\ntrain_fl_columns = [X1_column, X2_column, X3_column]\n\nfig, ax = plt.subplots()\nax.boxplot(train_fl_columns)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:37.299824Z","iopub.execute_input":"2021-06-28T09:02:37.300057Z","iopub.status.idle":"2021-06-28T09:02:37.449413Z","shell.execute_reply.started":"2021-06-28T09:02:37.300034Z","shell.execute_reply":"2021-06-28T09:02:37.448708Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# # Assigne des valeurs hors limites aux valeurs limites avec \"pandas.DataFrame.clip\"\n# X1_column = X1_column.clip(lower=X1_column.quantile(.01), upper=X1_column.quantile(.99))\n# # print(X1_column.shape)\n# # X1_column.plot.box()\n# X1_column = df_train_fl[\"X1\"]\n\n# X2_column = X2_column.clip(lower=X2_column.quantile(.01), upper=X2_column.quantile(.99))\n# X2_column = df_train_fl[\"X2\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:37.450848Z","iopub.execute_input":"2021-06-28T09:02:37.451232Z","iopub.status.idle":"2021-06-28T09:02:37.455400Z","shell.execute_reply.started":"2021-06-28T09:02:37.451189Z","shell.execute_reply":"2021-06-28T09:02:37.454376Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"<!-- #### \"stats.zscore\" ci-dessous calcule le Z-score de chaque valeur dans la colonne, par rapport à la moyenne de la colonne et à l’écart type.\n#### Lors du calcul du Z-score, nous redimensionnons et centrons les données et recherchons les points de données qui sont trop éloignés de zéro. Si la valeur du score Z est supérieure ou inférieure à 3 ou -3 respectivement, ce point de données sera identifié comme des valeurs aberrantes.  -->","metadata":{}},{"cell_type":"markdown","source":"#### **1. 3. 3. Mise à l'échelle de dataframes numériques de données d'apprentissage et expérimentales**","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0,1))\n\ncols = [\"X1\", \"X2\", \"X3\", \"X4\"]\n# transformation de données d'apprentissage\ndf_train_fl = pd.DataFrame(scaler.fit_transform(df_train_fl), columns = cols)\n# transformation de données expérimentales\ndf_test_fl = pd.DataFrame(scaler.fit_transform(df_test_fl), columns = cols)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:37.456957Z","iopub.execute_input":"2021-06-28T09:02:37.457341Z","iopub.status.idle":"2021-06-28T09:02:37.474010Z","shell.execute_reply.started":"2021-06-28T09:02:37.457301Z","shell.execute_reply":"2021-06-28T09:02:37.473184Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### **1. 4. Réduction de dimensionnalité de données d'apprentissage et expérimentales**","metadata":{}},{"cell_type":"markdown","source":"#### Tout en appliquant la réduction de dimensionnalité, nous ajustons d'abord le modèle à l'aide de données d'apprentissage, et ensuite nous pouvons utiliser pour transformer les données d'apprentissage et de validation. ","metadata":{}},{"cell_type":"markdown","source":"#### **1. 4. 1. SVD pour réduction de dimensionnalité des caractéristiques booléennes sparse** ","metadata":{}},{"cell_type":"markdown","source":"#### Séparation de la colonne de label de la base de données d'apprentissage booléenne:","metadata":{}},{"cell_type":"code","source":"label_train = df_train_bool.iloc[: , -1:] \n# label_train.shape # (1895, 1)\nprint(label_train.value_counts()) # 0: 1585, 1: 312\n\n# drop the \"outcome\" column (binary label column):\ndf_train_bool = df_train_bool.iloc[: , :-1] \ndf_train_bool.head()\ndf_train_bool.shape # (1895, 1556)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:37.474973Z","iopub.execute_input":"2021-06-28T09:02:37.475207Z","iopub.status.idle":"2021-06-28T09:02:37.498979Z","shell.execute_reply.started":"2021-06-28T09:02:37.475179Z","shell.execute_reply":"2021-06-28T09:02:37.497953Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"outcome\nnonad.     1585\nad.         312\ndtype: int64\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(1897, 1554)"},"metadata":{}}]},{"cell_type":"markdown","source":"#### **1. 4. 2. Optimisation de parametre \"n_components\" pour TruncatedSVD**","metadata":{}},{"cell_type":"markdown","source":"référence : https://chrisalbon.com/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd/","metadata":{}},{"cell_type":"code","source":"# Create and run an TSVD with one less than number of features\ntsvd = TruncatedSVD(n_components=df_train_bool.shape[1]-1)\nX_tsvd = tsvd.fit(df_train_bool)\n\n# List of explained variances\ntsvd_var_ratios = tsvd.explained_variance_ratio_\n\n# Calculating number of components required to pass threshold\ndef select_n_components(var_ratio, goal_var: float) -> int:\n\n    total_variance = 0.0\n    n_components = 0\n    \n    for explained_variance in var_ratio:\n        # Add the explained variance to the total\n        total_variance += explained_variance\n        # Add one to the number of components\n        n_components += 1\n        \n        if total_variance >= goal_var:\n            break\n            \n    return n_components\n\nselect_n_components(tsvd_var_ratios, 0.95) # 273","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:37.500304Z","iopub.execute_input":"2021-06-28T09:02:37.500664Z","iopub.status.idle":"2021-06-28T09:02:43.965456Z","shell.execute_reply.started":"2021-06-28T09:02:37.500634Z","shell.execute_reply":"2021-06-28T09:02:43.964529Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"273"},"metadata":{}}]},{"cell_type":"markdown","source":"#### **1. 4. 3. Application du codage SVD aux données d'apprentissage booléennes:**","metadata":{}},{"cell_type":"code","source":"SVD_model = TruncatedSVD(n_components=273).fit(df_train_bool)\nprint(df_train_bool.shape) # (1897, 734)\n\ndf_train_bool_reduced = pd.DataFrame(SVD_model.transform(df_train_bool)) # pd.DataFrame is used to retain as data frame object\nprint(df_train_bool_reduced.shape) # (1897, 290)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:43.967163Z","iopub.execute_input":"2021-06-28T09:02:43.967817Z","iopub.status.idle":"2021-06-28T09:02:45.638125Z","shell.execute_reply.started":"2021-06-28T09:02:43.967756Z","shell.execute_reply":"2021-06-28T09:02:45.636980Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(1897, 1554)\n(1897, 273)\n","output_type":"stream"}]},{"cell_type":"code","source":"# apply same transformation to df_test to boolean:\nprint(df_test_bool.shape) # (820, 1554)\ndf_test_bool_reduced = pd.DataFrame(SVD_model.transform(df_test_bool))\nprint(df_test_bool_reduced.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:45.642974Z","iopub.execute_input":"2021-06-28T09:02:45.645583Z","iopub.status.idle":"2021-06-28T09:02:45.702853Z","shell.execute_reply.started":"2021-06-28T09:02:45.645518Z","shell.execute_reply":"2021-06-28T09:02:45.701792Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(820, 1554)\n(820, 273)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Rassembler des dataframes de données d'expérimentales : ","metadata":{}},{"cell_type":"code","source":"# # les index ne correspondent pas après le prétraitement, il faut tomber:\n# df_test_bool_reduced.reset_index(drop=True, inplace=True)\n# df_test_fl.reset_index(drop=True, inplace=True)\n\n# # concaténation des jeux de données expérimentales booléens et numériques:\n# x_test = pd.concat( [df_test_fl, df_test_bool_reduced], axis=1 )\n# print((x_test.shape)) # (820, 27)\n# x_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:45.707613Z","iopub.execute_input":"2021-06-28T09:02:45.710221Z","iopub.status.idle":"2021-06-28T09:02:45.717267Z","shell.execute_reply.started":"2021-06-28T09:02:45.710150Z","shell.execute_reply":"2021-06-28T09:02:45.716232Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# les index ne correspondent pas après le prétraitement, il faut tomber:\ndf_test_bool.reset_index(drop=True, inplace=True)\ndf_test_fl.reset_index(drop=True, inplace=True)\n\n# concaténation des jeux de données expérimentales booléens et numériques:\nx_test = pd.concat( [df_test_fl, df_test_bool], axis=1 )\nprint((x_test.shape)) # (820, 27)\n# x_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:45.722200Z","iopub.execute_input":"2021-06-28T09:02:45.724735Z","iopub.status.idle":"2021-06-28T09:02:45.742938Z","shell.execute_reply.started":"2021-06-28T09:02:45.724678Z","shell.execute_reply":"2021-06-28T09:02:45.741941Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"(820, 1558)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **1. 5. Equilibrage des données largement en déséquilibre**","metadata":{}},{"cell_type":"markdown","source":"#### **1. 5. 1. Visualisation du ratio des classes**","metadata":{}},{"cell_type":"code","source":"target_count = df_train.outcome.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)')","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:45.747593Z","iopub.execute_input":"2021-06-28T09:02:45.749980Z","iopub.status.idle":"2021-06-28T09:02:45.919692Z","shell.execute_reply.started":"2021-06-28T09:02:45.749930Z","shell.execute_reply":"2021-06-28T09:02:45.918828Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Class 0: 1585\nClass 1: 312\nProportion: 5.08 : 1\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:title={'center':'Count (target)'}>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"#### **1. 5. 2. Application de plusieurs méthodes de suréchantillonnage et comparaison avec la validation croisée**","metadata":{}},{"cell_type":"markdown","source":"#### Ajustement des méthodes de suréchantillonnage \"SMOTE\", \"ADASYN\" et \"RandomOverSampler\" pour comparer avec la validation croisée 10-fold: ","metadata":{}},{"cell_type":"code","source":"oversample = RandomOverSampler(sampling_strategy='minority')\nx_over, y_over = oversample.fit_resample(df_train_bool, label_train)\n\nsm = SMOTE(random_state = 42)\nx_sm, y_sm = sm.fit_resample(df_train_bool, label_train)\n\nsm = ADASYN()\nx_syn, y_syn = sm.fit_resample(df_train_bool, label_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:45.921062Z","iopub.execute_input":"2021-06-28T09:02:45.921336Z","iopub.status.idle":"2021-06-28T09:02:47.268242Z","shell.execute_reply.started":"2021-06-28T09:02:45.921308Z","shell.execute_reply":"2021-06-28T09:02:47.267306Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# définir le pipeline \n# steps = [('svd', TruncatedSVD(n_components=272)), ('m', LogisticRegression())] \n# model = Pipeline(steps=steps)\n\nmodel = LogisticRegression()\n# évaluer le modèle \ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n\ny_pred_ROS = cross_val_predict(model, x_over, y_over, cv=cv, n_jobs=-1) # 0.97 recall for nonad\nprint(\"classification report after RandomOverSampler:\\n\", classification_report(y_over, y_pred_ROS))\n\ny_pred_sm = cross_val_predict(model, x_sm, y_sm, cv=cv, n_jobs=-1)\nprint(\"classification report after SMOTE:\\n\", classification_report(y_sm, y_pred_sm))\n\ny_pred_syn = cross_val_predict(model, x_syn, y_syn, cv=cv, n_jobs=-1)\nprint(\"classification report after ADASYN:\\n\", classification_report(y_syn, y_pred_syn))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:47.269673Z","iopub.execute_input":"2021-06-28T09:02:47.270016Z","iopub.status.idle":"2021-06-28T09:02:58.884439Z","shell.execute_reply.started":"2021-06-28T09:02:47.269983Z","shell.execute_reply":"2021-06-28T09:02:58.883448Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"classification report after RandomOverSampler:\n               precision    recall  f1-score   support\n\n         ad.       0.97      0.95      0.96      1585\n      nonad.       0.96      0.97      0.96      1585\n\n    accuracy                           0.96      3170\n   macro avg       0.96      0.96      0.96      3170\nweighted avg       0.96      0.96      0.96      3170\n\nclassification report after SMOTE:\n               precision    recall  f1-score   support\n\n         ad.       0.91      0.94      0.92      1585\n      nonad.       0.93      0.91      0.92      1585\n\n    accuracy                           0.92      3170\n   macro avg       0.92      0.92      0.92      3170\nweighted avg       0.92      0.92      0.92      3170\n\nclassification report after ADASYN:\n               precision    recall  f1-score   support\n\n         ad.       0.86      0.98      0.92      1563\n      nonad.       0.98      0.85      0.91      1585\n\n    accuracy                           0.91      3148\n   macro avg       0.92      0.91      0.91      3148\nweighted avg       0.92      0.91      0.91      3148\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### RandomOverSampler donne les meilleurs f1-scores sur le jeu de données booléen, alors il sera appliquer:","metadata":{}},{"cell_type":"code","source":"oversample = RandomOverSampler(sampling_strategy='minority')\n# suréchantillonnage de la trame de données d'apprentissage booléenne :\nx_over_bool, y_over = oversample.fit_resample(df_train_bool, label_train) \nprint((x_over_bool.shape), (y_over.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:58.886068Z","iopub.execute_input":"2021-06-28T09:02:58.886381Z","iopub.status.idle":"2021-06-28T09:02:59.203444Z","shell.execute_reply.started":"2021-06-28T09:02:58.886352Z","shell.execute_reply":"2021-06-28T09:02:59.202497Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"(3170, 1554) (3170, 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Tailles des trames de données booléen avant suréchantillonnage: (1897, 290), et après suréchantillonnage: (3170, 290)","metadata":{}},{"cell_type":"code","source":"# suréchantillonnage de la trame de données d'apprentissage numérique:\nx_over_fl, y_over = oversample.fit_resample(df_train_fl, label_train)\nprint((x_over_fl.shape), (y_over.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:59.204953Z","iopub.execute_input":"2021-06-28T09:02:59.205285Z","iopub.status.idle":"2021-06-28T09:02:59.222375Z","shell.execute_reply.started":"2021-06-28T09:02:59.205253Z","shell.execute_reply":"2021-06-28T09:02:59.221231Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"(3170, 4) (3170, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# concaténation de données d'apprentissage numérique et booléen:\nx_over = pd.concat( [x_over_fl, x_over_bool], axis=1 )\nprint((x_over.shape))\nx_over.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:59.224375Z","iopub.execute_input":"2021-06-28T09:02:59.224675Z","iopub.status.idle":"2021-06-28T09:02:59.273115Z","shell.execute_reply.started":"2021-06-28T09:02:59.224636Z","shell.execute_reply":"2021-06-28T09:02:59.271981Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"(3170, 1558)\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"         X1        X2        X3   X4  X5  X6  X7  X8  X9  X10  ...  X1549  \\\n0  0.048589  0.358372  0.116138  1.0   0   0   0   0   0    0  ...      0   \n1  0.090909  0.730829  0.129978  1.0   0   0   0   0   0    0  ...      0   \n2  0.090909  0.730829  0.129978  1.0   0   0   0   0   0    0  ...      0   \n3  0.089342  0.718310  0.129922  1.0   0   0   0   0   0    0  ...      0   \n4  0.090909  0.364632  0.064977  1.0   0   0   0   0   0    0  ...      0   \n\n   X1550  X1551  X1552  X1553  X1554  X1555  X1556  X1557  X1558  \n0      0      0      0      0      0      0      0      0      0  \n1      0      0      0      0      0      0      0      0      0  \n2      0      0      0      0      0      0      0      0      0  \n3      0      0      0      0      0      0      0      0      0  \n4      0      0      0      0      0      0      0      0      0  \n\n[5 rows x 1558 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n      <th>X7</th>\n      <th>X8</th>\n      <th>X9</th>\n      <th>X10</th>\n      <th>...</th>\n      <th>X1549</th>\n      <th>X1550</th>\n      <th>X1551</th>\n      <th>X1552</th>\n      <th>X1553</th>\n      <th>X1554</th>\n      <th>X1555</th>\n      <th>X1556</th>\n      <th>X1557</th>\n      <th>X1558</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.048589</td>\n      <td>0.358372</td>\n      <td>0.116138</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.090909</td>\n      <td>0.730829</td>\n      <td>0.129978</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.090909</td>\n      <td>0.730829</td>\n      <td>0.129978</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.089342</td>\n      <td>0.718310</td>\n      <td>0.129922</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.090909</td>\n      <td>0.364632</td>\n      <td>0.064977</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1558 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## **2. Évaluation des modèles de classification avec validation croisée 10-fold**","metadata":{}},{"cell_type":"markdown","source":"#### À chaque itération, \"cross_val_predict\" générera un score métrique individuel pour ce lot. En fin de compte, il renverra k score pour chaque itération. ","metadata":{}},{"cell_type":"code","source":"cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n\nmodel = LogisticRegression()\ny_pred_logr = cross_val_predict(model, x_over, y_over, cv=cv, n_jobs=-1) \nprint(\"classification report for logistic regression classifier:\\n\", classification_report(y_over, y_pred_logr))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:02:59.274608Z","iopub.execute_input":"2021-06-28T09:02:59.275029Z","iopub.status.idle":"2021-06-28T09:03:02.210222Z","shell.execute_reply.started":"2021-06-28T09:02:59.274986Z","shell.execute_reply":"2021-06-28T09:03:02.209095Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"classification report for logistic regression classifier:\n               precision    recall  f1-score   support\n\n         ad.       0.98      0.95      0.97      1585\n      nonad.       0.96      0.98      0.97      1585\n\n    accuracy                           0.97      3170\n   macro avg       0.97      0.97      0.97      3170\nweighted avg       0.97      0.97      0.97      3170\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model = DecisionTreeClassifier()\ny_pred_dt = cross_val_predict(model, x_over, y_over, cv=cv, n_jobs=-1) \nprint(\"classification report for decision tree classifier:\\n\", classification_report(y_over, y_pred_dt))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:03:02.216301Z","iopub.execute_input":"2021-06-28T09:03:02.216619Z","iopub.status.idle":"2021-06-28T09:03:04.389851Z","shell.execute_reply.started":"2021-06-28T09:03:02.216590Z","shell.execute_reply":"2021-06-28T09:03:04.388739Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"classification report for decision tree classifier:\n               precision    recall  f1-score   support\n\n         ad.       0.93      0.97      0.95      1585\n      nonad.       0.97      0.92      0.95      1585\n\n    accuracy                           0.95      3170\n   macro avg       0.95      0.95      0.95      3170\nweighted avg       0.95      0.95      0.95      3170\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Dans les matrices de confusion ci-dessus, les deux méthodes donnent le même score de *exactitude moyenne*. Étant donné que nos données ont un biais pour la classe minoritaire \"nonad\", nous devons comparer les valeurs de *rappel* et de *précision* pour être sûr que cette classe est correctement classée. Pour la classe non publicitaire, la valeur *rappel* renvoie 0,93 avec le classificateur *d'arbre de décision* et 0,98 avec le classificateur de *régression logistique*. Par conséquent, le classificateur de régression logistique surpasse le classificateur d'arbre de décision pour cet ensemble de données. ","metadata":{}},{"cell_type":"markdown","source":"## **3. Prédictions basées sur des modèles**","metadata":{}},{"cell_type":"markdown","source":"### **3. 1. Ajuster un modèle de régression logistique**","metadata":{}},{"cell_type":"code","source":"print(x_over.shape)\nprint(y_over.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:03:04.392444Z","iopub.execute_input":"2021-06-28T09:03:04.392753Z","iopub.status.idle":"2021-06-28T09:03:04.397484Z","shell.execute_reply.started":"2021-06-28T09:03:04.392720Z","shell.execute_reply":"2021-06-28T09:03:04.396651Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"(3170, 1558)\n(3170, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"logreg_model = LogisticRegression()\nlogreg_model.fit(x_over, y_over)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:03:04.398679Z","iopub.execute_input":"2021-06-28T09:03:04.399020Z","iopub.status.idle":"2021-06-28T09:03:04.711629Z","shell.execute_reply.started":"2021-06-28T09:03:04.398990Z","shell.execute_reply":"2021-06-28T09:03:04.710530Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(*args, **kwargs)\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"LogisticRegression()"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Prediction avec les données expérimentales :","metadata":{}},{"cell_type":"code","source":"y_pred_logreg = logreg_model.predict(x_test) # numpy.ndarray","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:03:04.713300Z","iopub.execute_input":"2021-06-28T09:03:04.713957Z","iopub.status.idle":"2021-06-28T09:03:04.752233Z","shell.execute_reply.started":"2021-06-28T09:03:04.713903Z","shell.execute_reply":"2021-06-28T09:03:04.751174Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# # enregistre dans un fichier CSV\n# savetxt('y_pred_logreg.csv', y_pred, delimiter=',', fmt=('%s'))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:03:04.753893Z","iopub.execute_input":"2021-06-28T09:03:04.754517Z","iopub.status.idle":"2021-06-28T09:03:04.758920Z","shell.execute_reply.started":"2021-06-28T09:03:04.754477Z","shell.execute_reply":"2021-06-28T09:03:04.757869Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### **3. 2. Ajuster un modèle de réseau neuronal profond**","metadata":{}},{"cell_type":"code","source":"# transformer la colonne de label des chaînes en entiers https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html \nlb = preprocessing.LabelBinarizer()\ny_over = lb.fit_transform(y_over) # 'numpy.ndarray' object ","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:03:04.760719Z","iopub.execute_input":"2021-06-28T09:03:04.761464Z","iopub.status.idle":"2021-06-28T09:03:04.793116Z","shell.execute_reply.started":"2021-06-28T09:03:04.761358Z","shell.execute_reply":"2021-06-28T09:03:04.792094Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Diviser les données d'apprentissage pour obtenir l’ensemble de données de validation :\nx_train, x_valid, y_train, y_valid = train_test_split(x_over, y_over, test_size=0.33, shuffle= True)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:03:04.794631Z","iopub.execute_input":"2021-06-28T09:03:04.795231Z","iopub.status.idle":"2021-06-28T09:03:04.850705Z","shell.execute_reply.started":"2021-06-28T09:03:04.795195Z","shell.execute_reply":"2021-06-28T09:03:04.849447Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# créer un modèle de perceptron multicouche avec nombre de couches 7:\ndef create_model():\n    model = Sequential()\n    model.add(Dropout(.4))\n    model.add(Dense(128, input_dim = x_train.shape[1], activation = 'sigmoid'))\n    model.add(Dropout(.4))\n    model.add(Dense(64, activation = 'relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(32, activation = 'relu'))\n    model.add(Dropout(.2))\n    model.add(Dense(16, activation = 'relu'))\n    model.add(Dropout(.1))\n    model.add(Dense(8, activation = 'relu'))\n    model.add(Dense(2, activation = 'relu'))\n    model.add(Dense(1, activation = 'sigmoid'))\n\n# compile model:\n    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = [tf.keras.metrics.Recall(), tf.keras.metrics.Precision(), tf.keras.metrics.Accuracy()])\n    # La perte d’entropie croisée binaire est utilisée pour l'applications de classification binaire (0 ou 1). \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:03:04.852519Z","iopub.execute_input":"2021-06-28T09:03:04.853280Z","iopub.status.idle":"2021-06-28T09:03:04.865970Z","shell.execute_reply.started":"2021-06-28T09:03:04.853222Z","shell.execute_reply":"2021-06-28T09:03:04.864864Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# évaluer le modèle avec un ensemble de données standardisé \nkeras_model = KerasClassifier(build_fn = create_model, verbose=1) \n\n# ajustement du modèle keras sur l'ensemble de données de validation\nhistory = keras_model.fit(x_train, y_train, \n                    epochs=400, \n                    batch_size=50, \n                    validation_data=(x_valid, y_valid), \n                    shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:03:04.867756Z","iopub.execute_input":"2021-06-28T09:03:04.868714Z","iopub.status.idle":"2021-06-28T09:04:45.188329Z","shell.execute_reply.started":"2021-06-28T09:03:04.868664Z","shell.execute_reply":"2021-06-28T09:04:45.187278Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Epoch 1/400\n43/43 [==============================] - 2s 24ms/step - loss: 0.6945 - recall: 0.5004 - precision: 0.4767 - accuracy: 0.0000e+00 - val_loss: 0.6749 - val_recall: 0.9387 - val_precision: 0.8673 - val_accuracy: 0.0000e+00\nEpoch 2/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.6763 - recall: 0.5055 - precision: 0.5951 - accuracy: 0.0000e+00 - val_loss: 0.5641 - val_recall: 0.6245 - val_precision: 0.9132 - val_accuracy: 0.0000e+00\nEpoch 3/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.5880 - recall: 0.7026 - precision: 0.7485 - accuracy: 0.0000e+00 - val_loss: 0.4445 - val_recall: 0.9789 - val_precision: 0.8474 - val_accuracy: 0.0000e+00\nEpoch 4/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.4820 - recall: 0.8770 - precision: 0.8024 - accuracy: 0.0000e+00 - val_loss: 0.4331 - val_recall: 0.9923 - val_precision: 0.7945 - val_accuracy: 0.0000e+00\nEpoch 5/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.4535 - recall: 0.9356 - precision: 0.7954 - accuracy: 0.0000e+00 - val_loss: 0.4205 - val_recall: 0.9981 - val_precision: 0.7979 - val_accuracy: 0.0000e+00\nEpoch 6/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.4416 - recall: 0.9575 - precision: 0.8025 - accuracy: 0.0000e+00 - val_loss: 0.3858 - val_recall: 0.9885 - val_precision: 0.8487 - val_accuracy: 0.0000e+00\nEpoch 7/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.4179 - recall: 0.9445 - precision: 0.8108 - accuracy: 0.0000e+00 - val_loss: 0.3906 - val_recall: 0.9981 - val_precision: 0.8179 - val_accuracy: 0.0000e+00\nEpoch 8/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.4052 - recall: 0.9705 - precision: 0.8295 - accuracy: 0.0000e+00 - val_loss: 0.3850 - val_recall: 0.9981 - val_precision: 0.8192 - val_accuracy: 0.0000e+00\nEpoch 9/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.3882 - recall: 0.9683 - precision: 0.8248 - accuracy: 0.0000e+00 - val_loss: 0.3666 - val_recall: 0.9981 - val_precision: 0.8376 - val_accuracy: 0.0000e+00\nEpoch 10/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.3865 - recall: 0.9463 - precision: 0.8533 - accuracy: 0.0000e+00 - val_loss: 0.3550 - val_recall: 0.9943 - val_precision: 0.8480 - val_accuracy: 0.0000e+00\nEpoch 11/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.3485 - recall: 0.9513 - precision: 0.8688 - accuracy: 0.0000e+00 - val_loss: 0.3453 - val_recall: 0.9943 - val_precision: 0.8494 - val_accuracy: 0.0000e+00\nEpoch 12/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.3351 - recall: 0.9599 - precision: 0.8828 - accuracy: 0.0000e+00 - val_loss: 0.3294 - val_recall: 0.9828 - val_precision: 0.8830 - val_accuracy: 0.0000e+00\nEpoch 13/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.3423 - recall: 0.9496 - precision: 0.8737 - accuracy: 0.0000e+00 - val_loss: 0.3195 - val_recall: 0.9866 - val_precision: 0.8925 - val_accuracy: 0.0000e+00\nEpoch 14/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.3542 - recall: 0.9611 - precision: 0.8581 - accuracy: 0.0000e+00 - val_loss: 0.3117 - val_recall: 0.9923 - val_precision: 0.8900 - val_accuracy: 0.0000e+00\nEpoch 15/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.3143 - recall: 0.9723 - precision: 0.8818 - accuracy: 0.0000e+00 - val_loss: 0.3024 - val_recall: 0.9962 - val_precision: 0.8950 - val_accuracy: 0.0000e+00\nEpoch 16/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.3005 - recall: 0.9688 - precision: 0.8981 - accuracy: 0.0000e+00 - val_loss: 0.2949 - val_recall: 0.9923 - val_precision: 0.8962 - val_accuracy: 0.0000e+00\nEpoch 17/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.3002 - recall: 0.9618 - precision: 0.8851 - accuracy: 0.0000e+00 - val_loss: 0.2802 - val_recall: 0.9866 - val_precision: 0.9180 - val_accuracy: 0.0000e+00\nEpoch 18/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.3066 - recall: 0.9448 - precision: 0.8781 - accuracy: 0.0000e+00 - val_loss: 0.2873 - val_recall: 0.9943 - val_precision: 0.8948 - val_accuracy: 0.0000e+00\nEpoch 19/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2959 - recall: 0.9627 - precision: 0.9043 - accuracy: 0.0000e+00 - val_loss: 0.2732 - val_recall: 0.9885 - val_precision: 0.9069 - val_accuracy: 0.0000e+00\nEpoch 20/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2975 - recall: 0.9627 - precision: 0.8879 - accuracy: 0.0000e+00 - val_loss: 0.2820 - val_recall: 0.9962 - val_precision: 0.8889 - val_accuracy: 0.0000e+00\nEpoch 21/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2818 - recall: 0.9748 - precision: 0.8946 - accuracy: 0.0000e+00 - val_loss: 0.2696 - val_recall: 0.9904 - val_precision: 0.8960 - val_accuracy: 0.0000e+00\nEpoch 22/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2765 - recall: 0.9689 - precision: 0.9016 - accuracy: 0.0000e+00 - val_loss: 0.2623 - val_recall: 0.9885 - val_precision: 0.9005 - val_accuracy: 0.0000e+00\nEpoch 23/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2733 - recall: 0.9728 - precision: 0.8953 - accuracy: 0.0000e+00 - val_loss: 0.2616 - val_recall: 0.9904 - val_precision: 0.8991 - val_accuracy: 0.0000e+00\nEpoch 24/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2668 - recall: 0.9692 - precision: 0.8964 - accuracy: 0.0000e+00 - val_loss: 0.2602 - val_recall: 0.9923 - val_precision: 0.8993 - val_accuracy: 0.0000e+00\nEpoch 25/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2728 - recall: 0.9676 - precision: 0.9004 - accuracy: 0.0000e+00 - val_loss: 0.2535 - val_recall: 0.9885 - val_precision: 0.9005 - val_accuracy: 0.0000e+00\nEpoch 26/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2421 - recall: 0.9788 - precision: 0.9232 - accuracy: 0.0000e+00 - val_loss: 0.2386 - val_recall: 0.9847 - val_precision: 0.9211 - val_accuracy: 0.0000e+00\nEpoch 27/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2591 - recall: 0.9456 - precision: 0.9071 - accuracy: 0.0000e+00 - val_loss: 0.2409 - val_recall: 0.9885 - val_precision: 0.9085 - val_accuracy: 0.0000e+00\nEpoch 28/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2403 - recall: 0.9736 - precision: 0.9040 - accuracy: 0.0000e+00 - val_loss: 0.2415 - val_recall: 0.9904 - val_precision: 0.9038 - val_accuracy: 0.0000e+00\nEpoch 29/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2525 - recall: 0.9704 - precision: 0.9045 - accuracy: 0.0000e+00 - val_loss: 0.2414 - val_recall: 0.9923 - val_precision: 0.9024 - val_accuracy: 0.0000e+00\nEpoch 30/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2508 - recall: 0.9790 - precision: 0.8928 - accuracy: 0.0000e+00 - val_loss: 0.2365 - val_recall: 0.9904 - val_precision: 0.9086 - val_accuracy: 0.0000e+00\nEpoch 31/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2405 - recall: 0.9765 - precision: 0.9096 - accuracy: 0.0000e+00 - val_loss: 0.2287 - val_recall: 0.9828 - val_precision: 0.9096 - val_accuracy: 0.0000e+00\nEpoch 32/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2537 - recall: 0.9577 - precision: 0.9151 - accuracy: 0.0000e+00 - val_loss: 0.2280 - val_recall: 0.9847 - val_precision: 0.9097 - val_accuracy: 0.0000e+00\nEpoch 33/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2354 - recall: 0.9600 - precision: 0.9095 - accuracy: 0.0000e+00 - val_loss: 0.2245 - val_recall: 0.9847 - val_precision: 0.9097 - val_accuracy: 0.0000e+00\nEpoch 34/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2408 - recall: 0.9683 - precision: 0.9002 - accuracy: 0.0000e+00 - val_loss: 0.2152 - val_recall: 0.9847 - val_precision: 0.9195 - val_accuracy: 0.0000e+00\nEpoch 35/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2245 - recall: 0.9663 - precision: 0.9123 - accuracy: 0.0000e+00 - val_loss: 0.2211 - val_recall: 0.9866 - val_precision: 0.9083 - val_accuracy: 0.0000e+00\nEpoch 36/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2250 - recall: 0.9775 - precision: 0.9057 - accuracy: 0.0000e+00 - val_loss: 0.2232 - val_recall: 0.9943 - val_precision: 0.9042 - val_accuracy: 0.0000e+00\nEpoch 37/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2105 - recall: 0.9840 - precision: 0.9230 - accuracy: 0.0000e+00 - val_loss: 0.2047 - val_recall: 0.9828 - val_precision: 0.9128 - val_accuracy: 0.0000e+00\nEpoch 38/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2139 - recall: 0.9662 - precision: 0.9237 - accuracy: 0.0000e+00 - val_loss: 0.2013 - val_recall: 0.9847 - val_precision: 0.9278 - val_accuracy: 0.0000e+00\nEpoch 39/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2160 - recall: 0.9597 - precision: 0.9321 - accuracy: 0.0000e+00 - val_loss: 0.2028 - val_recall: 0.9847 - val_precision: 0.9130 - val_accuracy: 0.0000e+00\nEpoch 40/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2188 - recall: 0.9710 - precision: 0.9139 - accuracy: 0.0000e+00 - val_loss: 0.1995 - val_recall: 0.9847 - val_precision: 0.9130 - val_accuracy: 0.0000e+00\nEpoch 41/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2024 - recall: 0.9694 - precision: 0.9252 - accuracy: 0.0000e+00 - val_loss: 0.1948 - val_recall: 0.9847 - val_precision: 0.9211 - val_accuracy: 0.0000e+00\nEpoch 42/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2264 - recall: 0.9690 - precision: 0.8992 - accuracy: 0.0000e+00 - val_loss: 0.1998 - val_recall: 0.9847 - val_precision: 0.9081 - val_accuracy: 0.0000e+00\nEpoch 43/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2092 - recall: 0.9767 - precision: 0.9117 - accuracy: 0.0000e+00 - val_loss: 0.1964 - val_recall: 0.9847 - val_precision: 0.9130 - val_accuracy: 0.0000e+00\nEpoch 44/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2066 - recall: 0.9670 - precision: 0.9178 - accuracy: 0.0000e+00 - val_loss: 0.2004 - val_recall: 0.9866 - val_precision: 0.9099 - val_accuracy: 0.0000e+00\nEpoch 45/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2028 - recall: 0.9767 - precision: 0.9112 - accuracy: 0.0000e+00 - val_loss: 0.1884 - val_recall: 0.9847 - val_precision: 0.9146 - val_accuracy: 0.0000e+00\nEpoch 46/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.2093 - recall: 0.9688 - precision: 0.9164 - accuracy: 0.0000e+00 - val_loss: 0.1917 - val_recall: 0.9847 - val_precision: 0.9130 - val_accuracy: 0.0000e+00\nEpoch 47/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1919 - recall: 0.9668 - precision: 0.9281 - accuracy: 0.0000e+00 - val_loss: 0.1860 - val_recall: 0.9847 - val_precision: 0.9162 - val_accuracy: 0.0000e+00\nEpoch 48/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.2040 - recall: 0.9670 - precision: 0.9082 - accuracy: 0.0000e+00 - val_loss: 0.1889 - val_recall: 0.9885 - val_precision: 0.9117 - val_accuracy: 0.0000e+00\nEpoch 49/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1858 - recall: 0.9805 - precision: 0.9232 - accuracy: 0.0000e+00 - val_loss: 0.1759 - val_recall: 0.9828 - val_precision: 0.9277 - val_accuracy: 0.0000e+00\nEpoch 50/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1779 - recall: 0.9722 - precision: 0.9367 - accuracy: 0.0000e+00 - val_loss: 0.1757 - val_recall: 0.9847 - val_precision: 0.9211 - val_accuracy: 0.0000e+00\nEpoch 51/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1884 - recall: 0.9640 - precision: 0.9229 - accuracy: 0.0000e+00 - val_loss: 0.1769 - val_recall: 0.9847 - val_precision: 0.9195 - val_accuracy: 0.0000e+00\nEpoch 52/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1854 - recall: 0.9617 - precision: 0.9295 - accuracy: 0.0000e+00 - val_loss: 0.1786 - val_recall: 0.9847 - val_precision: 0.9146 - val_accuracy: 0.0000e+00\nEpoch 53/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1761 - recall: 0.9790 - precision: 0.9246 - accuracy: 0.0000e+00 - val_loss: 0.1731 - val_recall: 0.9847 - val_precision: 0.9146 - val_accuracy: 0.0000e+00\nEpoch 54/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1913 - recall: 0.9692 - precision: 0.9253 - accuracy: 0.0000e+00 - val_loss: 0.1702 - val_recall: 0.9847 - val_precision: 0.9295 - val_accuracy: 0.0000e+00\nEpoch 55/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1931 - recall: 0.9699 - precision: 0.9254 - accuracy: 0.0000e+00 - val_loss: 0.1689 - val_recall: 0.9808 - val_precision: 0.9343 - val_accuracy: 0.0000e+00\nEpoch 56/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1724 - recall: 0.9753 - precision: 0.9381 - accuracy: 0.0000e+00 - val_loss: 0.1756 - val_recall: 0.9866 - val_precision: 0.9147 - val_accuracy: 0.0000e+00\nEpoch 57/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1793 - recall: 0.9779 - precision: 0.9195 - accuracy: 0.0000e+00 - val_loss: 0.1694 - val_recall: 0.9847 - val_precision: 0.9195 - val_accuracy: 0.0000e+00\nEpoch 58/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1665 - recall: 0.9739 - precision: 0.9285 - accuracy: 0.0000e+00 - val_loss: 0.1645 - val_recall: 0.9828 - val_precision: 0.9277 - val_accuracy: 0.0000e+00\nEpoch 59/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1917 - recall: 0.9716 - precision: 0.9180 - accuracy: 0.0000e+00 - val_loss: 0.1628 - val_recall: 0.9828 - val_precision: 0.9293 - val_accuracy: 0.0000e+00\nEpoch 60/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1735 - recall: 0.9696 - precision: 0.9294 - accuracy: 0.0000e+00 - val_loss: 0.1659 - val_recall: 0.9828 - val_precision: 0.9227 - val_accuracy: 0.0000e+00\nEpoch 61/400\n43/43 [==============================] - 0s 7ms/step - loss: 0.1669 - recall: 0.9725 - precision: 0.9321 - accuracy: 0.0000e+00 - val_loss: 0.1664 - val_recall: 0.9828 - val_precision: 0.9227 - val_accuracy: 0.0000e+00\nEpoch 62/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1732 - recall: 0.9758 - precision: 0.9217 - accuracy: 0.0000e+00 - val_loss: 0.1714 - val_recall: 0.9847 - val_precision: 0.9162 - val_accuracy: 0.0000e+00\nEpoch 63/400\n43/43 [==============================] - 0s 7ms/step - loss: 0.1792 - recall: 0.9805 - precision: 0.9123 - accuracy: 0.0000e+00 - val_loss: 0.1716 - val_recall: 0.9885 - val_precision: 0.9149 - val_accuracy: 0.0000e+00\nEpoch 64/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1595 - recall: 0.9852 - precision: 0.9310 - accuracy: 0.0000e+00 - val_loss: 0.1634 - val_recall: 0.9866 - val_precision: 0.9246 - val_accuracy: 0.0000e+00\nEpoch 65/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1686 - recall: 0.9813 - precision: 0.9247 - accuracy: 0.0000e+00 - val_loss: 0.1578 - val_recall: 0.9828 - val_precision: 0.9293 - val_accuracy: 0.0000e+00\nEpoch 66/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1594 - recall: 0.9612 - precision: 0.9362 - accuracy: 0.0000e+00 - val_loss: 0.1634 - val_recall: 0.9847 - val_precision: 0.9211 - val_accuracy: 0.0000e+00\nEpoch 67/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1614 - recall: 0.9793 - precision: 0.9284 - accuracy: 0.0000e+00 - val_loss: 0.1630 - val_recall: 0.9847 - val_precision: 0.9228 - val_accuracy: 0.0000e+00\nEpoch 68/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1722 - recall: 0.9809 - precision: 0.9212 - accuracy: 0.0000e+00 - val_loss: 0.1584 - val_recall: 0.9828 - val_precision: 0.9260 - val_accuracy: 0.0000e+00\nEpoch 69/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1713 - recall: 0.9699 - precision: 0.9248 - accuracy: 0.0000e+00 - val_loss: 0.1582 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 70/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1700 - recall: 0.9791 - precision: 0.9240 - accuracy: 0.0000e+00 - val_loss: 0.1548 - val_recall: 0.9828 - val_precision: 0.9293 - val_accuracy: 0.0000e+00\nEpoch 71/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1662 - recall: 0.9722 - precision: 0.9312 - accuracy: 0.0000e+00 - val_loss: 0.1546 - val_recall: 0.9847 - val_precision: 0.9211 - val_accuracy: 0.0000e+00\nEpoch 72/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1594 - recall: 0.9751 - precision: 0.9243 - accuracy: 0.0000e+00 - val_loss: 0.1514 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 73/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1569 - recall: 0.9736 - precision: 0.9327 - accuracy: 0.0000e+00 - val_loss: 0.1499 - val_recall: 0.9751 - val_precision: 0.9496 - val_accuracy: 0.0000e+00\nEpoch 74/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1653 - recall: 0.9647 - precision: 0.9288 - accuracy: 0.0000e+00 - val_loss: 0.1544 - val_recall: 0.9828 - val_precision: 0.9210 - val_accuracy: 0.0000e+00\nEpoch 75/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1534 - recall: 0.9835 - precision: 0.9303 - accuracy: 0.0000e+00 - val_loss: 0.1484 - val_recall: 0.9828 - val_precision: 0.9293 - val_accuracy: 0.0000e+00\nEpoch 76/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1571 - recall: 0.9754 - precision: 0.9243 - accuracy: 0.0000e+00 - val_loss: 0.1466 - val_recall: 0.9828 - val_precision: 0.9293 - val_accuracy: 0.0000e+00\nEpoch 77/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1362 - recall: 0.9695 - precision: 0.9477 - accuracy: 0.0000e+00 - val_loss: 0.1577 - val_recall: 0.9847 - val_precision: 0.9195 - val_accuracy: 0.0000e+00\nEpoch 78/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1720 - recall: 0.9743 - precision: 0.9175 - accuracy: 0.0000e+00 - val_loss: 0.1575 - val_recall: 0.9847 - val_precision: 0.9162 - val_accuracy: 0.0000e+00\nEpoch 79/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1585 - recall: 0.9731 - precision: 0.9308 - accuracy: 0.0000e+00 - val_loss: 0.1466 - val_recall: 0.9847 - val_precision: 0.9261 - val_accuracy: 0.0000e+00\nEpoch 80/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1587 - recall: 0.9760 - precision: 0.9238 - accuracy: 0.0000e+00 - val_loss: 0.1486 - val_recall: 0.9847 - val_precision: 0.9228 - val_accuracy: 0.0000e+00\nEpoch 81/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1476 - recall: 0.9736 - precision: 0.9398 - accuracy: 0.0000e+00 - val_loss: 0.1486 - val_recall: 0.9828 - val_precision: 0.9293 - val_accuracy: 0.0000e+00\nEpoch 82/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1641 - recall: 0.9601 - precision: 0.9416 - accuracy: 0.0000e+00 - val_loss: 0.1483 - val_recall: 0.9828 - val_precision: 0.9277 - val_accuracy: 0.0000e+00\nEpoch 83/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1697 - recall: 0.9615 - precision: 0.9347 - accuracy: 0.0000e+00 - val_loss: 0.1467 - val_recall: 0.9847 - val_precision: 0.9245 - val_accuracy: 0.0000e+00\nEpoch 84/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1549 - recall: 0.9823 - precision: 0.9269 - accuracy: 0.0000e+00 - val_loss: 0.1473 - val_recall: 0.9847 - val_precision: 0.9245 - val_accuracy: 0.0000e+00\nEpoch 85/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1453 - recall: 0.9815 - precision: 0.9492 - accuracy: 0.0000e+00 - val_loss: 0.1478 - val_recall: 0.9847 - val_precision: 0.9278 - val_accuracy: 0.0000e+00\nEpoch 86/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1643 - recall: 0.9786 - precision: 0.9088 - accuracy: 0.0000e+00 - val_loss: 0.1452 - val_recall: 0.9847 - val_precision: 0.9261 - val_accuracy: 0.0000e+00\nEpoch 87/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1500 - recall: 0.9746 - precision: 0.9328 - accuracy: 0.0000e+00 - val_loss: 0.1454 - val_recall: 0.9828 - val_precision: 0.9260 - val_accuracy: 0.0000e+00\nEpoch 88/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1430 - recall: 0.9733 - precision: 0.9365 - accuracy: 0.0000e+00 - val_loss: 0.1474 - val_recall: 0.9847 - val_precision: 0.9245 - val_accuracy: 0.0000e+00\nEpoch 89/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1492 - recall: 0.9805 - precision: 0.9273 - accuracy: 0.0000e+00 - val_loss: 0.1479 - val_recall: 0.9847 - val_precision: 0.9195 - val_accuracy: 0.0000e+00\nEpoch 90/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1449 - recall: 0.9726 - precision: 0.9264 - accuracy: 0.0000e+00 - val_loss: 0.1487 - val_recall: 0.9866 - val_precision: 0.9180 - val_accuracy: 0.0000e+00\nEpoch 91/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1483 - recall: 0.9803 - precision: 0.9275 - accuracy: 0.0000e+00 - val_loss: 0.1457 - val_recall: 0.9847 - val_precision: 0.9228 - val_accuracy: 0.0000e+00\nEpoch 92/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1572 - recall: 0.9719 - precision: 0.9261 - accuracy: 0.0000e+00 - val_loss: 0.1443 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 93/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1494 - recall: 0.9703 - precision: 0.9342 - accuracy: 0.0000e+00 - val_loss: 0.1470 - val_recall: 0.9847 - val_precision: 0.9211 - val_accuracy: 0.0000e+00\nEpoch 94/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1404 - recall: 0.9776 - precision: 0.9266 - accuracy: 0.0000e+00 - val_loss: 0.1399 - val_recall: 0.9828 - val_precision: 0.9310 - val_accuracy: 0.0000e+00\nEpoch 95/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1428 - recall: 0.9781 - precision: 0.9329 - accuracy: 0.0000e+00 - val_loss: 0.1418 - val_recall: 0.9847 - val_precision: 0.9245 - val_accuracy: 0.0000e+00\nEpoch 96/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1268 - recall: 0.9733 - precision: 0.9475 - accuracy: 0.0000e+00 - val_loss: 0.1418 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 97/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1377 - recall: 0.9797 - precision: 0.9396 - accuracy: 0.0000e+00 - val_loss: 0.1420 - val_recall: 0.9866 - val_precision: 0.9213 - val_accuracy: 0.0000e+00\nEpoch 98/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1490 - recall: 0.9798 - precision: 0.9305 - accuracy: 0.0000e+00 - val_loss: 0.1391 - val_recall: 0.9828 - val_precision: 0.9293 - val_accuracy: 0.0000e+00\nEpoch 99/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1390 - recall: 0.9721 - precision: 0.9290 - accuracy: 0.0000e+00 - val_loss: 0.1395 - val_recall: 0.9828 - val_precision: 0.9293 - val_accuracy: 0.0000e+00\nEpoch 100/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1618 - recall: 0.9739 - precision: 0.9275 - accuracy: 0.0000e+00 - val_loss: 0.1369 - val_recall: 0.9789 - val_precision: 0.9308 - val_accuracy: 0.0000e+00\nEpoch 101/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1543 - recall: 0.9646 - precision: 0.9289 - accuracy: 0.0000e+00 - val_loss: 0.1357 - val_recall: 0.9828 - val_precision: 0.9310 - val_accuracy: 0.0000e+00\nEpoch 102/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1342 - recall: 0.9723 - precision: 0.9479 - accuracy: 0.0000e+00 - val_loss: 0.1397 - val_recall: 0.9847 - val_precision: 0.9179 - val_accuracy: 0.0000e+00\nEpoch 103/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1420 - recall: 0.9797 - precision: 0.9265 - accuracy: 0.0000e+00 - val_loss: 0.1371 - val_recall: 0.9828 - val_precision: 0.9310 - val_accuracy: 0.0000e+00\nEpoch 104/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1639 - recall: 0.9716 - precision: 0.9088 - accuracy: 0.0000e+00 - val_loss: 0.1371 - val_recall: 0.9847 - val_precision: 0.9228 - val_accuracy: 0.0000e+00\nEpoch 105/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1427 - recall: 0.9723 - precision: 0.9344 - accuracy: 0.0000e+00 - val_loss: 0.1345 - val_recall: 0.9732 - val_precision: 0.9390 - val_accuracy: 0.0000e+00\nEpoch 106/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1314 - recall: 0.9741 - precision: 0.9458 - accuracy: 0.0000e+00 - val_loss: 0.1382 - val_recall: 0.9847 - val_precision: 0.9179 - val_accuracy: 0.0000e+00\nEpoch 107/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1328 - recall: 0.9773 - precision: 0.9352 - accuracy: 0.0000e+00 - val_loss: 0.1361 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 108/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1366 - recall: 0.9783 - precision: 0.9312 - accuracy: 0.0000e+00 - val_loss: 0.1355 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 109/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1307 - recall: 0.9779 - precision: 0.9465 - accuracy: 0.0000e+00 - val_loss: 0.1368 - val_recall: 0.9828 - val_precision: 0.9227 - val_accuracy: 0.0000e+00\nEpoch 110/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1174 - recall: 0.9746 - precision: 0.9509 - accuracy: 0.0000e+00 - val_loss: 0.1359 - val_recall: 0.9808 - val_precision: 0.9309 - val_accuracy: 0.0000e+00\nEpoch 111/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1478 - recall: 0.9659 - precision: 0.9353 - accuracy: 0.0000e+00 - val_loss: 0.1350 - val_recall: 0.9808 - val_precision: 0.9309 - val_accuracy: 0.0000e+00\nEpoch 112/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1541 - recall: 0.9762 - precision: 0.9219 - accuracy: 0.0000e+00 - val_loss: 0.1324 - val_recall: 0.9770 - val_precision: 0.9307 - val_accuracy: 0.0000e+00\nEpoch 113/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1496 - recall: 0.9660 - precision: 0.9280 - accuracy: 0.0000e+00 - val_loss: 0.1348 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 114/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1385 - recall: 0.9788 - precision: 0.9311 - accuracy: 0.0000e+00 - val_loss: 0.1391 - val_recall: 0.9866 - val_precision: 0.9147 - val_accuracy: 0.0000e+00\nEpoch 115/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1238 - recall: 0.9872 - precision: 0.9347 - accuracy: 0.0000e+00 - val_loss: 0.1363 - val_recall: 0.9808 - val_precision: 0.9309 - val_accuracy: 0.0000e+00\nEpoch 116/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1481 - recall: 0.9644 - precision: 0.9301 - accuracy: 0.0000e+00 - val_loss: 0.1362 - val_recall: 0.9847 - val_precision: 0.9228 - val_accuracy: 0.0000e+00\nEpoch 117/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1549 - recall: 0.9783 - precision: 0.9076 - accuracy: 0.0000e+00 - val_loss: 0.1372 - val_recall: 0.9866 - val_precision: 0.9180 - val_accuracy: 0.0000e+00\nEpoch 118/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1419 - recall: 0.9753 - precision: 0.9327 - accuracy: 0.0000e+00 - val_loss: 0.1350 - val_recall: 0.9866 - val_precision: 0.9213 - val_accuracy: 0.0000e+00\nEpoch 119/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1392 - recall: 0.9778 - precision: 0.9287 - accuracy: 0.0000e+00 - val_loss: 0.1335 - val_recall: 0.9847 - val_precision: 0.9245 - val_accuracy: 0.0000e+00\nEpoch 120/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1410 - recall: 0.9765 - precision: 0.9243 - accuracy: 0.0000e+00 - val_loss: 0.1306 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 121/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1269 - recall: 0.9703 - precision: 0.9375 - accuracy: 0.0000e+00 - val_loss: 0.1323 - val_recall: 0.9828 - val_precision: 0.9277 - val_accuracy: 0.0000e+00\nEpoch 122/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1454 - recall: 0.9650 - precision: 0.9361 - accuracy: 0.0000e+00 - val_loss: 0.1345 - val_recall: 0.9866 - val_precision: 0.9196 - val_accuracy: 0.0000e+00\nEpoch 123/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1221 - recall: 0.9913 - precision: 0.9361 - accuracy: 0.0000e+00 - val_loss: 0.1308 - val_recall: 0.9828 - val_precision: 0.9293 - val_accuracy: 0.0000e+00\nEpoch 124/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1309 - recall: 0.9758 - precision: 0.9371 - accuracy: 0.0000e+00 - val_loss: 0.1316 - val_recall: 0.9828 - val_precision: 0.9194 - val_accuracy: 0.0000e+00\nEpoch 125/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1421 - recall: 0.9736 - precision: 0.9293 - accuracy: 0.0000e+00 - val_loss: 0.1326 - val_recall: 0.9789 - val_precision: 0.9342 - val_accuracy: 0.0000e+00\nEpoch 126/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1485 - recall: 0.9664 - precision: 0.9302 - accuracy: 0.0000e+00 - val_loss: 0.1311 - val_recall: 0.9828 - val_precision: 0.9293 - val_accuracy: 0.0000e+00\nEpoch 127/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1363 - recall: 0.9718 - precision: 0.9398 - accuracy: 0.0000e+00 - val_loss: 0.1377 - val_recall: 0.9885 - val_precision: 0.9165 - val_accuracy: 0.0000e+00\nEpoch 128/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1420 - recall: 0.9801 - precision: 0.9202 - accuracy: 0.0000e+00 - val_loss: 0.1355 - val_recall: 0.9847 - val_precision: 0.9195 - val_accuracy: 0.0000e+00\nEpoch 129/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1217 - recall: 0.9791 - precision: 0.9301 - accuracy: 0.0000e+00 - val_loss: 0.1346 - val_recall: 0.9828 - val_precision: 0.9293 - val_accuracy: 0.0000e+00\nEpoch 130/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1280 - recall: 0.9764 - precision: 0.9491 - accuracy: 0.0000e+00 - val_loss: 0.1412 - val_recall: 0.9847 - val_precision: 0.9195 - val_accuracy: 0.0000e+00\nEpoch 131/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1218 - recall: 0.9771 - precision: 0.9348 - accuracy: 0.0000e+00 - val_loss: 0.1317 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 132/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1517 - recall: 0.9650 - precision: 0.9304 - accuracy: 0.0000e+00 - val_loss: 0.1313 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 133/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1199 - recall: 0.9787 - precision: 0.9428 - accuracy: 0.0000e+00 - val_loss: 0.1370 - val_recall: 0.9828 - val_precision: 0.9177 - val_accuracy: 0.0000e+00\nEpoch 134/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1308 - recall: 0.9853 - precision: 0.9290 - accuracy: 0.0000e+00 - val_loss: 0.1374 - val_recall: 0.9847 - val_precision: 0.9211 - val_accuracy: 0.0000e+00\nEpoch 135/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1151 - recall: 0.9898 - precision: 0.9366 - accuracy: 0.0000e+00 - val_loss: 0.1350 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 136/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1375 - recall: 0.9807 - precision: 0.9260 - accuracy: 0.0000e+00 - val_loss: 0.1373 - val_recall: 0.9866 - val_precision: 0.9180 - val_accuracy: 0.0000e+00\nEpoch 137/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1221 - recall: 0.9817 - precision: 0.9260 - accuracy: 0.0000e+00 - val_loss: 0.1356 - val_recall: 0.9866 - val_precision: 0.9164 - val_accuracy: 0.0000e+00\nEpoch 138/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1172 - recall: 0.9873 - precision: 0.9322 - accuracy: 0.0000e+00 - val_loss: 0.1328 - val_recall: 0.9866 - val_precision: 0.9164 - val_accuracy: 0.0000e+00\nEpoch 139/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1501 - recall: 0.9786 - precision: 0.9260 - accuracy: 0.0000e+00 - val_loss: 0.1282 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 140/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1284 - recall: 0.9709 - precision: 0.9276 - accuracy: 0.0000e+00 - val_loss: 0.1328 - val_recall: 0.9866 - val_precision: 0.9180 - val_accuracy: 0.0000e+00\nEpoch 141/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1365 - recall: 0.9725 - precision: 0.9199 - accuracy: 0.0000e+00 - val_loss: 0.1280 - val_recall: 0.9847 - val_precision: 0.9261 - val_accuracy: 0.0000e+00\nEpoch 142/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1259 - recall: 0.9779 - precision: 0.9352 - accuracy: 0.0000e+00 - val_loss: 0.1277 - val_recall: 0.9847 - val_precision: 0.9261 - val_accuracy: 0.0000e+00\nEpoch 143/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1213 - recall: 0.9783 - precision: 0.9281 - accuracy: 0.0000e+00 - val_loss: 0.1285 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 144/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1115 - recall: 0.9839 - precision: 0.9413 - accuracy: 0.0000e+00 - val_loss: 0.1360 - val_recall: 0.9866 - val_precision: 0.9147 - val_accuracy: 0.0000e+00\nEpoch 145/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1218 - recall: 0.9816 - precision: 0.9319 - accuracy: 0.0000e+00 - val_loss: 0.1260 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 146/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1218 - recall: 0.9740 - precision: 0.9460 - accuracy: 0.0000e+00 - val_loss: 0.1301 - val_recall: 0.9866 - val_precision: 0.9213 - val_accuracy: 0.0000e+00\nEpoch 147/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1541 - recall: 0.9732 - precision: 0.9115 - accuracy: 0.0000e+00 - val_loss: 0.1275 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 148/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0970 - recall: 0.9827 - precision: 0.9527 - accuracy: 0.0000e+00 - val_loss: 0.1270 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 149/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1183 - recall: 0.9742 - precision: 0.9420 - accuracy: 0.0000e+00 - val_loss: 0.1263 - val_recall: 0.9828 - val_precision: 0.9260 - val_accuracy: 0.0000e+00\nEpoch 150/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1299 - recall: 0.9753 - precision: 0.9348 - accuracy: 0.0000e+00 - val_loss: 0.1289 - val_recall: 0.9847 - val_precision: 0.9228 - val_accuracy: 0.0000e+00\nEpoch 151/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1414 - recall: 0.9831 - precision: 0.9170 - accuracy: 0.0000e+00 - val_loss: 0.1256 - val_recall: 0.9828 - val_precision: 0.9260 - val_accuracy: 0.0000e+00\nEpoch 152/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1321 - recall: 0.9766 - precision: 0.9340 - accuracy: 0.0000e+00 - val_loss: 0.1227 - val_recall: 0.9828 - val_precision: 0.9277 - val_accuracy: 0.0000e+00\nEpoch 153/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1238 - recall: 0.9828 - precision: 0.9373 - accuracy: 0.0000e+00 - val_loss: 0.1237 - val_recall: 0.9847 - val_precision: 0.9278 - val_accuracy: 0.0000e+00\nEpoch 154/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1324 - recall: 0.9676 - precision: 0.9186 - accuracy: 0.0000e+00 - val_loss: 0.1240 - val_recall: 0.9847 - val_precision: 0.9278 - val_accuracy: 0.0000e+00\nEpoch 155/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1258 - recall: 0.9833 - precision: 0.9331 - accuracy: 0.0000e+00 - val_loss: 0.1193 - val_recall: 0.9789 - val_precision: 0.9308 - val_accuracy: 0.0000e+00\nEpoch 156/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1122 - recall: 0.9740 - precision: 0.9467 - accuracy: 0.0000e+00 - val_loss: 0.1177 - val_recall: 0.9789 - val_precision: 0.9308 - val_accuracy: 0.0000e+00\nEpoch 157/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1157 - recall: 0.9700 - precision: 0.9506 - accuracy: 0.0000e+00 - val_loss: 0.1190 - val_recall: 0.9847 - val_precision: 0.9278 - val_accuracy: 0.0000e+00\nEpoch 158/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1348 - recall: 0.9739 - precision: 0.9236 - accuracy: 0.0000e+00 - val_loss: 0.1206 - val_recall: 0.9847 - val_precision: 0.9228 - val_accuracy: 0.0000e+00\nEpoch 159/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1127 - recall: 0.9844 - precision: 0.9352 - accuracy: 0.0000e+00 - val_loss: 0.1191 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 160/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1208 - recall: 0.9648 - precision: 0.9469 - accuracy: 0.0000e+00 - val_loss: 0.1258 - val_recall: 0.9866 - val_precision: 0.9180 - val_accuracy: 0.0000e+00\nEpoch 161/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1304 - recall: 0.9764 - precision: 0.9218 - accuracy: 0.0000e+00 - val_loss: 0.1222 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 162/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1221 - recall: 0.9729 - precision: 0.9322 - accuracy: 0.0000e+00 - val_loss: 0.1210 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 163/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1379 - recall: 0.9743 - precision: 0.9342 - accuracy: 0.0000e+00 - val_loss: 0.1207 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 164/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1166 - recall: 0.9807 - precision: 0.9431 - accuracy: 0.0000e+00 - val_loss: 0.1177 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 165/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1183 - recall: 0.9733 - precision: 0.9391 - accuracy: 0.0000e+00 - val_loss: 0.1229 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 166/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1262 - recall: 0.9681 - precision: 0.9335 - accuracy: 0.0000e+00 - val_loss: 0.1251 - val_recall: 0.9828 - val_precision: 0.9260 - val_accuracy: 0.0000e+00\nEpoch 167/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1173 - recall: 0.9770 - precision: 0.9398 - accuracy: 0.0000e+00 - val_loss: 0.1243 - val_recall: 0.9828 - val_precision: 0.9277 - val_accuracy: 0.0000e+00\nEpoch 168/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1078 - recall: 0.9807 - precision: 0.9438 - accuracy: 0.0000e+00 - val_loss: 0.1235 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 169/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1306 - recall: 0.9716 - precision: 0.9362 - accuracy: 0.0000e+00 - val_loss: 0.1261 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 170/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1182 - recall: 0.9783 - precision: 0.9408 - accuracy: 0.0000e+00 - val_loss: 0.1256 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 171/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1257 - recall: 0.9774 - precision: 0.9371 - accuracy: 0.0000e+00 - val_loss: 0.1228 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 172/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1207 - recall: 0.9755 - precision: 0.9305 - accuracy: 0.0000e+00 - val_loss: 0.1215 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 173/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1345 - recall: 0.9787 - precision: 0.9279 - accuracy: 0.0000e+00 - val_loss: 0.1205 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 174/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1062 - recall: 0.9817 - precision: 0.9449 - accuracy: 0.0000e+00 - val_loss: 0.1215 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 175/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1451 - recall: 0.9732 - precision: 0.9279 - accuracy: 0.0000e+00 - val_loss: 0.1217 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 176/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1290 - recall: 0.9668 - precision: 0.9279 - accuracy: 0.0000e+00 - val_loss: 0.1306 - val_recall: 0.9904 - val_precision: 0.9086 - val_accuracy: 0.0000e+00\nEpoch 177/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1383 - recall: 0.9930 - precision: 0.8980 - accuracy: 0.0000e+00 - val_loss: 0.1211 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 178/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1164 - recall: 0.9740 - precision: 0.9416 - accuracy: 0.0000e+00 - val_loss: 0.1176 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 179/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1131 - recall: 0.9843 - precision: 0.9342 - accuracy: 0.0000e+00 - val_loss: 0.1188 - val_recall: 0.9828 - val_precision: 0.9260 - val_accuracy: 0.0000e+00\nEpoch 180/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1196 - recall: 0.9806 - precision: 0.9265 - accuracy: 0.0000e+00 - val_loss: 0.1205 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 181/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1158 - recall: 0.9739 - precision: 0.9430 - accuracy: 0.0000e+00 - val_loss: 0.1220 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 182/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0992 - recall: 0.9861 - precision: 0.9459 - accuracy: 0.0000e+00 - val_loss: 0.1213 - val_recall: 0.9808 - val_precision: 0.9242 - val_accuracy: 0.0000e+00\nEpoch 183/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1077 - recall: 0.9833 - precision: 0.9340 - accuracy: 0.0000e+00 - val_loss: 0.1186 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 184/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1139 - recall: 0.9802 - precision: 0.9404 - accuracy: 0.0000e+00 - val_loss: 0.1227 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 185/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1247 - recall: 0.9810 - precision: 0.9311 - accuracy: 0.0000e+00 - val_loss: 0.1218 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 186/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1270 - recall: 0.9788 - precision: 0.9397 - accuracy: 0.0000e+00 - val_loss: 0.1218 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 187/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1259 - recall: 0.9804 - precision: 0.9239 - accuracy: 0.0000e+00 - val_loss: 0.1234 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 188/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1112 - recall: 0.9778 - precision: 0.9458 - accuracy: 0.0000e+00 - val_loss: 0.1184 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 189/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0921 - recall: 0.9860 - precision: 0.9477 - accuracy: 0.0000e+00 - val_loss: 0.1163 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 190/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1108 - recall: 0.9821 - precision: 0.9276 - accuracy: 0.0000e+00 - val_loss: 0.1147 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 191/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1101 - recall: 0.9703 - precision: 0.9513 - accuracy: 0.0000e+00 - val_loss: 0.1173 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 192/400\n43/43 [==============================] - 0s 7ms/step - loss: 0.1186 - recall: 0.9784 - precision: 0.9363 - accuracy: 0.0000e+00 - val_loss: 0.1154 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 193/400\n43/43 [==============================] - 0s 7ms/step - loss: 0.1118 - recall: 0.9848 - precision: 0.9336 - accuracy: 0.0000e+00 - val_loss: 0.1160 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 194/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1085 - recall: 0.9782 - precision: 0.9362 - accuracy: 0.0000e+00 - val_loss: 0.1145 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 195/400\n43/43 [==============================] - 0s 7ms/step - loss: 0.1043 - recall: 0.9744 - precision: 0.9487 - accuracy: 0.0000e+00 - val_loss: 0.1155 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 196/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1133 - recall: 0.9766 - precision: 0.9393 - accuracy: 0.0000e+00 - val_loss: 0.1147 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 197/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1300 - recall: 0.9723 - precision: 0.9303 - accuracy: 0.0000e+00 - val_loss: 0.1171 - val_recall: 0.9808 - val_precision: 0.9242 - val_accuracy: 0.0000e+00\nEpoch 198/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1213 - recall: 0.9746 - precision: 0.9373 - accuracy: 0.0000e+00 - val_loss: 0.1156 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 199/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1040 - recall: 0.9811 - precision: 0.9465 - accuracy: 0.0000e+00 - val_loss: 0.1198 - val_recall: 0.9847 - val_precision: 0.9179 - val_accuracy: 0.0000e+00\nEpoch 200/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1045 - recall: 0.9856 - precision: 0.9485 - accuracy: 0.0000e+00 - val_loss: 0.1168 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 201/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1137 - recall: 0.9808 - precision: 0.9298 - accuracy: 0.0000e+00 - val_loss: 0.1150 - val_recall: 0.9808 - val_precision: 0.9242 - val_accuracy: 0.0000e+00\nEpoch 202/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1207 - recall: 0.9703 - precision: 0.9353 - accuracy: 0.0000e+00 - val_loss: 0.1141 - val_recall: 0.9808 - val_precision: 0.9242 - val_accuracy: 0.0000e+00\nEpoch 203/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0958 - recall: 0.9798 - precision: 0.9513 - accuracy: 0.0000e+00 - val_loss: 0.1182 - val_recall: 0.9808 - val_precision: 0.9394 - val_accuracy: 0.0000e+00\nEpoch 204/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1169 - recall: 0.9758 - precision: 0.9415 - accuracy: 0.0000e+00 - val_loss: 0.1179 - val_recall: 0.9808 - val_precision: 0.9394 - val_accuracy: 0.0000e+00\nEpoch 205/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1002 - recall: 0.9741 - precision: 0.9458 - accuracy: 0.0000e+00 - val_loss: 0.1162 - val_recall: 0.9808 - val_precision: 0.9377 - val_accuracy: 0.0000e+00\nEpoch 206/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0962 - recall: 0.9829 - precision: 0.9505 - accuracy: 0.0000e+00 - val_loss: 0.1151 - val_recall: 0.9808 - val_precision: 0.9225 - val_accuracy: 0.0000e+00\nEpoch 207/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1307 - recall: 0.9840 - precision: 0.9197 - accuracy: 0.0000e+00 - val_loss: 0.1175 - val_recall: 0.9847 - val_precision: 0.9162 - val_accuracy: 0.0000e+00\nEpoch 208/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1329 - recall: 0.9805 - precision: 0.9232 - accuracy: 0.0000e+00 - val_loss: 0.1156 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 209/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1249 - recall: 0.9792 - precision: 0.9349 - accuracy: 0.0000e+00 - val_loss: 0.1148 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 210/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1116 - recall: 0.9767 - precision: 0.9439 - accuracy: 0.0000e+00 - val_loss: 0.1104 - val_recall: 0.9808 - val_precision: 0.9394 - val_accuracy: 0.0000e+00\nEpoch 211/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1125 - recall: 0.9737 - precision: 0.9343 - accuracy: 0.0000e+00 - val_loss: 0.1146 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 212/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1290 - recall: 0.9777 - precision: 0.9235 - accuracy: 0.0000e+00 - val_loss: 0.1137 - val_recall: 0.9808 - val_precision: 0.9309 - val_accuracy: 0.0000e+00\nEpoch 213/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1293 - recall: 0.9724 - precision: 0.9363 - accuracy: 0.0000e+00 - val_loss: 0.1129 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 214/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0994 - recall: 0.9725 - precision: 0.9548 - accuracy: 0.0000e+00 - val_loss: 0.1100 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 215/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1257 - recall: 0.9656 - precision: 0.9385 - accuracy: 0.0000e+00 - val_loss: 0.1131 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 216/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1058 - recall: 0.9829 - precision: 0.9481 - accuracy: 0.0000e+00 - val_loss: 0.1150 - val_recall: 0.9808 - val_precision: 0.9242 - val_accuracy: 0.0000e+00\nEpoch 217/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0982 - recall: 0.9808 - precision: 0.9460 - accuracy: 0.0000e+00 - val_loss: 0.1119 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 218/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1204 - recall: 0.9820 - precision: 0.9304 - accuracy: 0.0000e+00 - val_loss: 0.1136 - val_recall: 0.9808 - val_precision: 0.9343 - val_accuracy: 0.0000e+00\nEpoch 219/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1268 - recall: 0.9659 - precision: 0.9255 - accuracy: 0.0000e+00 - val_loss: 0.1147 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 220/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1260 - recall: 0.9818 - precision: 0.9164 - accuracy: 0.0000e+00 - val_loss: 0.1167 - val_recall: 0.9847 - val_precision: 0.9162 - val_accuracy: 0.0000e+00\nEpoch 221/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1223 - recall: 0.9839 - precision: 0.9124 - accuracy: 0.0000e+00 - val_loss: 0.1166 - val_recall: 0.9847 - val_precision: 0.9146 - val_accuracy: 0.0000e+00\nEpoch 222/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1172 - recall: 0.9802 - precision: 0.9303 - accuracy: 0.0000e+00 - val_loss: 0.1113 - val_recall: 0.9808 - val_precision: 0.9377 - val_accuracy: 0.0000e+00\nEpoch 223/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1129 - recall: 0.9695 - precision: 0.9339 - accuracy: 0.0000e+00 - val_loss: 0.1093 - val_recall: 0.9808 - val_precision: 0.9394 - val_accuracy: 0.0000e+00\nEpoch 224/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1131 - recall: 0.9826 - precision: 0.9403 - accuracy: 0.0000e+00 - val_loss: 0.1129 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 225/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1091 - recall: 0.9774 - precision: 0.9450 - accuracy: 0.0000e+00 - val_loss: 0.1136 - val_recall: 0.9808 - val_precision: 0.9309 - val_accuracy: 0.0000e+00\nEpoch 226/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0985 - recall: 0.9789 - precision: 0.9537 - accuracy: 0.0000e+00 - val_loss: 0.1130 - val_recall: 0.9847 - val_precision: 0.9211 - val_accuracy: 0.0000e+00\nEpoch 227/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1006 - recall: 0.9826 - precision: 0.9382 - accuracy: 0.0000e+00 - val_loss: 0.1110 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 228/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0971 - recall: 0.9760 - precision: 0.9450 - accuracy: 0.0000e+00 - val_loss: 0.1100 - val_recall: 0.9808 - val_precision: 0.9360 - val_accuracy: 0.0000e+00\nEpoch 229/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1087 - recall: 0.9666 - precision: 0.9472 - accuracy: 0.0000e+00 - val_loss: 0.1096 - val_recall: 0.9808 - val_precision: 0.9360 - val_accuracy: 0.0000e+00\nEpoch 230/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1049 - recall: 0.9805 - precision: 0.9356 - accuracy: 0.0000e+00 - val_loss: 0.1128 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 231/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1122 - recall: 0.9807 - precision: 0.9260 - accuracy: 0.0000e+00 - val_loss: 0.1144 - val_recall: 0.9847 - val_precision: 0.9228 - val_accuracy: 0.0000e+00\nEpoch 232/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1190 - recall: 0.9856 - precision: 0.9298 - accuracy: 0.0000e+00 - val_loss: 0.1106 - val_recall: 0.9808 - val_precision: 0.9309 - val_accuracy: 0.0000e+00\nEpoch 233/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1092 - recall: 0.9791 - precision: 0.9366 - accuracy: 0.0000e+00 - val_loss: 0.1078 - val_recall: 0.9808 - val_precision: 0.9360 - val_accuracy: 0.0000e+00\nEpoch 234/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0989 - recall: 0.9833 - precision: 0.9520 - accuracy: 0.0000e+00 - val_loss: 0.1106 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 235/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1098 - recall: 0.9748 - precision: 0.9408 - accuracy: 0.0000e+00 - val_loss: 0.1072 - val_recall: 0.9808 - val_precision: 0.9446 - val_accuracy: 0.0000e+00\nEpoch 236/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1218 - recall: 0.9679 - precision: 0.9281 - accuracy: 0.0000e+00 - val_loss: 0.1089 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 237/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1065 - recall: 0.9769 - precision: 0.9319 - accuracy: 0.0000e+00 - val_loss: 0.1084 - val_recall: 0.9808 - val_precision: 0.9343 - val_accuracy: 0.0000e+00\nEpoch 238/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1065 - recall: 0.9745 - precision: 0.9479 - accuracy: 0.0000e+00 - val_loss: 0.1099 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 239/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0899 - recall: 0.9730 - precision: 0.9464 - accuracy: 0.0000e+00 - val_loss: 0.1070 - val_recall: 0.9808 - val_precision: 0.9412 - val_accuracy: 0.0000e+00\nEpoch 240/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1040 - recall: 0.9816 - precision: 0.9467 - accuracy: 0.0000e+00 - val_loss: 0.1092 - val_recall: 0.9808 - val_precision: 0.9309 - val_accuracy: 0.0000e+00\nEpoch 241/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0936 - recall: 0.9878 - precision: 0.9540 - accuracy: 0.0000e+00 - val_loss: 0.1094 - val_recall: 0.9808 - val_precision: 0.9360 - val_accuracy: 0.0000e+00\nEpoch 242/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0979 - recall: 0.9813 - precision: 0.9507 - accuracy: 0.0000e+00 - val_loss: 0.1061 - val_recall: 0.9789 - val_precision: 0.9463 - val_accuracy: 0.0000e+00\nEpoch 243/400\n43/43 [==============================] - 0s 7ms/step - loss: 0.1083 - recall: 0.9688 - precision: 0.9484 - accuracy: 0.0000e+00 - val_loss: 0.1062 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 244/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1136 - recall: 0.9811 - precision: 0.9343 - accuracy: 0.0000e+00 - val_loss: 0.1082 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 245/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1062 - recall: 0.9813 - precision: 0.9388 - accuracy: 0.0000e+00 - val_loss: 0.1076 - val_recall: 0.9808 - val_precision: 0.9209 - val_accuracy: 0.0000e+00\nEpoch 246/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1192 - recall: 0.9802 - precision: 0.9325 - accuracy: 0.0000e+00 - val_loss: 0.1076 - val_recall: 0.9808 - val_precision: 0.9360 - val_accuracy: 0.0000e+00\nEpoch 247/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1151 - recall: 0.9685 - precision: 0.9338 - accuracy: 0.0000e+00 - val_loss: 0.1101 - val_recall: 0.9847 - val_precision: 0.9162 - val_accuracy: 0.0000e+00\nEpoch 248/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1096 - recall: 0.9821 - precision: 0.9241 - accuracy: 0.0000e+00 - val_loss: 0.1037 - val_recall: 0.9789 - val_precision: 0.9428 - val_accuracy: 0.0000e+00\nEpoch 249/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1128 - recall: 0.9809 - precision: 0.9454 - accuracy: 0.0000e+00 - val_loss: 0.1089 - val_recall: 0.9808 - val_precision: 0.9377 - val_accuracy: 0.0000e+00\nEpoch 250/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1053 - recall: 0.9799 - precision: 0.9409 - accuracy: 0.0000e+00 - val_loss: 0.1074 - val_recall: 0.9808 - val_precision: 0.9412 - val_accuracy: 0.0000e+00\nEpoch 251/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1039 - recall: 0.9828 - precision: 0.9433 - accuracy: 0.0000e+00 - val_loss: 0.1066 - val_recall: 0.9808 - val_precision: 0.9429 - val_accuracy: 0.0000e+00\nEpoch 252/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0908 - recall: 0.9835 - precision: 0.9490 - accuracy: 0.0000e+00 - val_loss: 0.1077 - val_recall: 0.9789 - val_precision: 0.9445 - val_accuracy: 0.0000e+00\nEpoch 253/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1021 - recall: 0.9804 - precision: 0.9406 - accuracy: 0.0000e+00 - val_loss: 0.1086 - val_recall: 0.9808 - val_precision: 0.9343 - val_accuracy: 0.0000e+00\nEpoch 254/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0935 - recall: 0.9817 - precision: 0.9494 - accuracy: 0.0000e+00 - val_loss: 0.1085 - val_recall: 0.9828 - val_precision: 0.9277 - val_accuracy: 0.0000e+00\nEpoch 255/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0981 - recall: 0.9894 - precision: 0.9450 - accuracy: 0.0000e+00 - val_loss: 0.1074 - val_recall: 0.9808 - val_precision: 0.9394 - val_accuracy: 0.0000e+00\nEpoch 256/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1192 - recall: 0.9788 - precision: 0.9386 - accuracy: 0.0000e+00 - val_loss: 0.1080 - val_recall: 0.9808 - val_precision: 0.9360 - val_accuracy: 0.0000e+00\nEpoch 257/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0965 - recall: 0.9828 - precision: 0.9468 - accuracy: 0.0000e+00 - val_loss: 0.1075 - val_recall: 0.9789 - val_precision: 0.9463 - val_accuracy: 0.0000e+00\nEpoch 258/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1190 - recall: 0.9759 - precision: 0.9368 - accuracy: 0.0000e+00 - val_loss: 0.1075 - val_recall: 0.9808 - val_precision: 0.9242 - val_accuracy: 0.0000e+00\nEpoch 259/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1053 - recall: 0.9785 - precision: 0.9379 - accuracy: 0.0000e+00 - val_loss: 0.1064 - val_recall: 0.9789 - val_precision: 0.9393 - val_accuracy: 0.0000e+00\nEpoch 260/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1013 - recall: 0.9776 - precision: 0.9504 - accuracy: 0.0000e+00 - val_loss: 0.1064 - val_recall: 0.9808 - val_precision: 0.9309 - val_accuracy: 0.0000e+00\nEpoch 261/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1127 - recall: 0.9833 - precision: 0.9414 - accuracy: 0.0000e+00 - val_loss: 0.1047 - val_recall: 0.9789 - val_precision: 0.9359 - val_accuracy: 0.0000e+00\nEpoch 262/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0982 - recall: 0.9830 - precision: 0.9549 - accuracy: 0.0000e+00 - val_loss: 0.1085 - val_recall: 0.9789 - val_precision: 0.9359 - val_accuracy: 0.0000e+00\nEpoch 263/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1003 - recall: 0.9820 - precision: 0.9446 - accuracy: 0.0000e+00 - val_loss: 0.1088 - val_recall: 0.9808 - val_precision: 0.9209 - val_accuracy: 0.0000e+00\nEpoch 264/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1134 - recall: 0.9847 - precision: 0.9299 - accuracy: 0.0000e+00 - val_loss: 0.1067 - val_recall: 0.9789 - val_precision: 0.9445 - val_accuracy: 0.0000e+00\nEpoch 265/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0989 - recall: 0.9816 - precision: 0.9465 - accuracy: 0.0000e+00 - val_loss: 0.1070 - val_recall: 0.9789 - val_precision: 0.9428 - val_accuracy: 0.0000e+00\nEpoch 266/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0969 - recall: 0.9727 - precision: 0.9465 - accuracy: 0.0000e+00 - val_loss: 0.1093 - val_recall: 0.9789 - val_precision: 0.9325 - val_accuracy: 0.0000e+00\nEpoch 267/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1023 - recall: 0.9811 - precision: 0.9442 - accuracy: 0.0000e+00 - val_loss: 0.1091 - val_recall: 0.9789 - val_precision: 0.9411 - val_accuracy: 0.0000e+00\nEpoch 268/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1116 - recall: 0.9839 - precision: 0.9354 - accuracy: 0.0000e+00 - val_loss: 0.1082 - val_recall: 0.9789 - val_precision: 0.9393 - val_accuracy: 0.0000e+00\nEpoch 269/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1102 - recall: 0.9737 - precision: 0.9312 - accuracy: 0.0000e+00 - val_loss: 0.1098 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 270/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1103 - recall: 0.9841 - precision: 0.9352 - accuracy: 0.0000e+00 - val_loss: 0.1102 - val_recall: 0.9808 - val_precision: 0.9309 - val_accuracy: 0.0000e+00\nEpoch 271/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1130 - recall: 0.9713 - precision: 0.9405 - accuracy: 0.0000e+00 - val_loss: 0.1074 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 272/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1149 - recall: 0.9797 - precision: 0.9310 - accuracy: 0.0000e+00 - val_loss: 0.1113 - val_recall: 0.9828 - val_precision: 0.9194 - val_accuracy: 0.0000e+00\nEpoch 273/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0998 - recall: 0.9791 - precision: 0.9376 - accuracy: 0.0000e+00 - val_loss: 0.1109 - val_recall: 0.9789 - val_precision: 0.9308 - val_accuracy: 0.0000e+00\nEpoch 274/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0998 - recall: 0.9848 - precision: 0.9415 - accuracy: 0.0000e+00 - val_loss: 0.1126 - val_recall: 0.9789 - val_precision: 0.9445 - val_accuracy: 0.0000e+00\nEpoch 275/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1019 - recall: 0.9696 - precision: 0.9555 - accuracy: 0.0000e+00 - val_loss: 0.1121 - val_recall: 0.9789 - val_precision: 0.9359 - val_accuracy: 0.0000e+00\nEpoch 276/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0999 - recall: 0.9787 - precision: 0.9482 - accuracy: 0.0000e+00 - val_loss: 0.1108 - val_recall: 0.9828 - val_precision: 0.9210 - val_accuracy: 0.0000e+00\nEpoch 277/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1100 - recall: 0.9822 - precision: 0.9358 - accuracy: 0.0000e+00 - val_loss: 0.1097 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 278/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0981 - recall: 0.9790 - precision: 0.9459 - accuracy: 0.0000e+00 - val_loss: 0.1087 - val_recall: 0.9789 - val_precision: 0.9481 - val_accuracy: 0.0000e+00\nEpoch 279/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1285 - recall: 0.9755 - precision: 0.9422 - accuracy: 0.0000e+00 - val_loss: 0.1076 - val_recall: 0.9847 - val_precision: 0.9179 - val_accuracy: 0.0000e+00\nEpoch 280/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1111 - recall: 0.9748 - precision: 0.9346 - accuracy: 0.0000e+00 - val_loss: 0.1048 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 281/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1020 - recall: 0.9779 - precision: 0.9360 - accuracy: 0.0000e+00 - val_loss: 0.1047 - val_recall: 0.9789 - val_precision: 0.9393 - val_accuracy: 0.0000e+00\nEpoch 282/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1014 - recall: 0.9776 - precision: 0.9450 - accuracy: 0.0000e+00 - val_loss: 0.1053 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 283/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0979 - recall: 0.9893 - precision: 0.9430 - accuracy: 0.0000e+00 - val_loss: 0.1120 - val_recall: 0.9808 - val_precision: 0.9377 - val_accuracy: 0.0000e+00\nEpoch 284/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1133 - recall: 0.9830 - precision: 0.9374 - accuracy: 0.0000e+00 - val_loss: 0.1095 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 285/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0816 - recall: 0.9834 - precision: 0.9610 - accuracy: 0.0000e+00 - val_loss: 0.1078 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 286/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0974 - recall: 0.9880 - precision: 0.9356 - accuracy: 0.0000e+00 - val_loss: 0.1074 - val_recall: 0.9789 - val_precision: 0.9463 - val_accuracy: 0.0000e+00\nEpoch 287/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0989 - recall: 0.9750 - precision: 0.9510 - accuracy: 0.0000e+00 - val_loss: 0.1085 - val_recall: 0.9808 - val_precision: 0.9360 - val_accuracy: 0.0000e+00\nEpoch 288/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0920 - recall: 0.9789 - precision: 0.9551 - accuracy: 0.0000e+00 - val_loss: 0.1057 - val_recall: 0.9789 - val_precision: 0.9428 - val_accuracy: 0.0000e+00\nEpoch 289/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1033 - recall: 0.9750 - precision: 0.9399 - accuracy: 0.0000e+00 - val_loss: 0.1031 - val_recall: 0.9789 - val_precision: 0.9393 - val_accuracy: 0.0000e+00\nEpoch 290/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1063 - recall: 0.9777 - precision: 0.9455 - accuracy: 0.0000e+00 - val_loss: 0.1052 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 291/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1111 - recall: 0.9836 - precision: 0.9431 - accuracy: 0.0000e+00 - val_loss: 0.1077 - val_recall: 0.9789 - val_precision: 0.9359 - val_accuracy: 0.0000e+00\nEpoch 292/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0963 - recall: 0.9819 - precision: 0.9397 - accuracy: 0.0000e+00 - val_loss: 0.1076 - val_recall: 0.9789 - val_precision: 0.9428 - val_accuracy: 0.0000e+00\nEpoch 293/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0925 - recall: 0.9837 - precision: 0.9429 - accuracy: 0.0000e+00 - val_loss: 0.1112 - val_recall: 0.9789 - val_precision: 0.9393 - val_accuracy: 0.0000e+00\nEpoch 294/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1200 - recall: 0.9710 - precision: 0.9352 - accuracy: 0.0000e+00 - val_loss: 0.1076 - val_recall: 0.9828 - val_precision: 0.9227 - val_accuracy: 0.0000e+00\nEpoch 295/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1036 - recall: 0.9793 - precision: 0.9302 - accuracy: 0.0000e+00 - val_loss: 0.1054 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 296/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1244 - recall: 0.9787 - precision: 0.9185 - accuracy: 0.0000e+00 - val_loss: 0.1079 - val_recall: 0.9789 - val_precision: 0.9445 - val_accuracy: 0.0000e+00\nEpoch 297/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0935 - recall: 0.9858 - precision: 0.9528 - accuracy: 0.0000e+00 - val_loss: 0.1102 - val_recall: 0.9789 - val_precision: 0.9359 - val_accuracy: 0.0000e+00\nEpoch 298/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0960 - recall: 0.9826 - precision: 0.9428 - accuracy: 0.0000e+00 - val_loss: 0.1109 - val_recall: 0.9847 - val_precision: 0.9162 - val_accuracy: 0.0000e+00\nEpoch 299/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1080 - recall: 0.9799 - precision: 0.9287 - accuracy: 0.0000e+00 - val_loss: 0.1082 - val_recall: 0.9828 - val_precision: 0.9260 - val_accuracy: 0.0000e+00\nEpoch 300/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1038 - recall: 0.9809 - precision: 0.9473 - accuracy: 0.0000e+00 - val_loss: 0.1090 - val_recall: 0.9789 - val_precision: 0.9274 - val_accuracy: 0.0000e+00\nEpoch 301/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0974 - recall: 0.9803 - precision: 0.9439 - accuracy: 0.0000e+00 - val_loss: 0.1101 - val_recall: 0.9789 - val_precision: 0.9257 - val_accuracy: 0.0000e+00\nEpoch 302/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1098 - recall: 0.9742 - precision: 0.9361 - accuracy: 0.0000e+00 - val_loss: 0.1065 - val_recall: 0.9789 - val_precision: 0.9463 - val_accuracy: 0.0000e+00\nEpoch 303/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1040 - recall: 0.9763 - precision: 0.9386 - accuracy: 0.0000e+00 - val_loss: 0.1076 - val_recall: 0.9789 - val_precision: 0.9463 - val_accuracy: 0.0000e+00\nEpoch 304/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0985 - recall: 0.9788 - precision: 0.9509 - accuracy: 0.0000e+00 - val_loss: 0.1041 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 305/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1089 - recall: 0.9760 - precision: 0.9481 - accuracy: 0.0000e+00 - val_loss: 0.1065 - val_recall: 0.9789 - val_precision: 0.9463 - val_accuracy: 0.0000e+00\nEpoch 306/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1024 - recall: 0.9718 - precision: 0.9383 - accuracy: 0.0000e+00 - val_loss: 0.1033 - val_recall: 0.9847 - val_precision: 0.9245 - val_accuracy: 0.0000e+00\nEpoch 307/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1078 - recall: 0.9796 - precision: 0.9303 - accuracy: 0.0000e+00 - val_loss: 0.1012 - val_recall: 0.9789 - val_precision: 0.9445 - val_accuracy: 0.0000e+00\nEpoch 308/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0992 - recall: 0.9854 - precision: 0.9377 - accuracy: 0.0000e+00 - val_loss: 0.1041 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 309/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1010 - recall: 0.9864 - precision: 0.9396 - accuracy: 0.0000e+00 - val_loss: 0.1066 - val_recall: 0.9789 - val_precision: 0.9428 - val_accuracy: 0.0000e+00\nEpoch 310/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1156 - recall: 0.9740 - precision: 0.9400 - accuracy: 0.0000e+00 - val_loss: 0.1085 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 311/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0952 - recall: 0.9841 - precision: 0.9443 - accuracy: 0.0000e+00 - val_loss: 0.1099 - val_recall: 0.9828 - val_precision: 0.9227 - val_accuracy: 0.0000e+00\nEpoch 312/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1069 - recall: 0.9825 - precision: 0.9252 - accuracy: 0.0000e+00 - val_loss: 0.1057 - val_recall: 0.9808 - val_precision: 0.9343 - val_accuracy: 0.0000e+00\nEpoch 313/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0959 - recall: 0.9787 - precision: 0.9483 - accuracy: 0.0000e+00 - val_loss: 0.1083 - val_recall: 0.9789 - val_precision: 0.9445 - val_accuracy: 0.0000e+00\nEpoch 314/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0940 - recall: 0.9760 - precision: 0.9514 - accuracy: 0.0000e+00 - val_loss: 0.1053 - val_recall: 0.9828 - val_precision: 0.9243 - val_accuracy: 0.0000e+00\nEpoch 315/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1015 - recall: 0.9835 - precision: 0.9433 - accuracy: 0.0000e+00 - val_loss: 0.1057 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 316/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0894 - recall: 0.9904 - precision: 0.9490 - accuracy: 0.0000e+00 - val_loss: 0.1036 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 317/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1049 - recall: 0.9779 - precision: 0.9403 - accuracy: 0.0000e+00 - val_loss: 0.1039 - val_recall: 0.9789 - val_precision: 0.9481 - val_accuracy: 0.0000e+00\nEpoch 318/400\n43/43 [==============================] - 0s 7ms/step - loss: 0.1020 - recall: 0.9829 - precision: 0.9459 - accuracy: 0.0000e+00 - val_loss: 0.1042 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 319/400\n43/43 [==============================] - 0s 7ms/step - loss: 0.1042 - recall: 0.9834 - precision: 0.9397 - accuracy: 0.0000e+00 - val_loss: 0.1043 - val_recall: 0.9789 - val_precision: 0.9411 - val_accuracy: 0.0000e+00\nEpoch 320/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0909 - recall: 0.9858 - precision: 0.9540 - accuracy: 0.0000e+00 - val_loss: 0.1057 - val_recall: 0.9789 - val_precision: 0.9359 - val_accuracy: 0.0000e+00\nEpoch 321/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1064 - recall: 0.9842 - precision: 0.9300 - accuracy: 0.0000e+00 - val_loss: 0.1065 - val_recall: 0.9789 - val_precision: 0.9428 - val_accuracy: 0.0000e+00\nEpoch 322/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1029 - recall: 0.9792 - precision: 0.9435 - accuracy: 0.0000e+00 - val_loss: 0.1061 - val_recall: 0.9808 - val_precision: 0.9225 - val_accuracy: 0.0000e+00\nEpoch 323/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1108 - recall: 0.9804 - precision: 0.9293 - accuracy: 0.0000e+00 - val_loss: 0.1088 - val_recall: 0.9808 - val_precision: 0.9225 - val_accuracy: 0.0000e+00\nEpoch 324/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1078 - recall: 0.9796 - precision: 0.9279 - accuracy: 0.0000e+00 - val_loss: 0.1064 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 325/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0930 - recall: 0.9935 - precision: 0.9475 - accuracy: 0.0000e+00 - val_loss: 0.1075 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 326/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1028 - recall: 0.9834 - precision: 0.9264 - accuracy: 0.0000e+00 - val_loss: 0.1074 - val_recall: 0.9789 - val_precision: 0.9308 - val_accuracy: 0.0000e+00\nEpoch 327/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0869 - recall: 0.9788 - precision: 0.9568 - accuracy: 0.0000e+00 - val_loss: 0.1063 - val_recall: 0.9789 - val_precision: 0.9393 - val_accuracy: 0.0000e+00\nEpoch 328/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0958 - recall: 0.9753 - precision: 0.9496 - accuracy: 0.0000e+00 - val_loss: 0.1044 - val_recall: 0.9789 - val_precision: 0.9393 - val_accuracy: 0.0000e+00\nEpoch 329/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0865 - recall: 0.9808 - precision: 0.9554 - accuracy: 0.0000e+00 - val_loss: 0.1059 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 330/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1050 - recall: 0.9721 - precision: 0.9484 - accuracy: 0.0000e+00 - val_loss: 0.1050 - val_recall: 0.9808 - val_precision: 0.9394 - val_accuracy: 0.0000e+00\nEpoch 331/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1095 - recall: 0.9865 - precision: 0.9309 - accuracy: 0.0000e+00 - val_loss: 0.1085 - val_recall: 0.9789 - val_precision: 0.9445 - val_accuracy: 0.0000e+00\nEpoch 332/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1042 - recall: 0.9737 - precision: 0.9363 - accuracy: 0.0000e+00 - val_loss: 0.1076 - val_recall: 0.9808 - val_precision: 0.9412 - val_accuracy: 0.0000e+00\nEpoch 333/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1117 - recall: 0.9836 - precision: 0.9444 - accuracy: 0.0000e+00 - val_loss: 0.1070 - val_recall: 0.9808 - val_precision: 0.9429 - val_accuracy: 0.0000e+00\nEpoch 334/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1011 - recall: 0.9748 - precision: 0.9450 - accuracy: 0.0000e+00 - val_loss: 0.1060 - val_recall: 0.9808 - val_precision: 0.9394 - val_accuracy: 0.0000e+00\nEpoch 335/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0886 - recall: 0.9926 - precision: 0.9464 - accuracy: 0.0000e+00 - val_loss: 0.1047 - val_recall: 0.9789 - val_precision: 0.9428 - val_accuracy: 0.0000e+00\nEpoch 336/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1038 - recall: 0.9814 - precision: 0.9397 - accuracy: 0.0000e+00 - val_loss: 0.1051 - val_recall: 0.9808 - val_precision: 0.9309 - val_accuracy: 0.0000e+00\nEpoch 337/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0849 - recall: 0.9813 - precision: 0.9425 - accuracy: 0.0000e+00 - val_loss: 0.1031 - val_recall: 0.9808 - val_precision: 0.9309 - val_accuracy: 0.0000e+00\nEpoch 338/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0906 - recall: 0.9867 - precision: 0.9396 - accuracy: 0.0000e+00 - val_loss: 0.1065 - val_recall: 0.9789 - val_precision: 0.9445 - val_accuracy: 0.0000e+00\nEpoch 339/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1015 - recall: 0.9756 - precision: 0.9429 - accuracy: 0.0000e+00 - val_loss: 0.1072 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 340/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1078 - recall: 0.9800 - precision: 0.9320 - accuracy: 0.0000e+00 - val_loss: 0.1093 - val_recall: 0.9789 - val_precision: 0.9428 - val_accuracy: 0.0000e+00\nEpoch 341/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1018 - recall: 0.9835 - precision: 0.9453 - accuracy: 0.0000e+00 - val_loss: 0.1103 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 342/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0816 - recall: 0.9875 - precision: 0.9555 - accuracy: 0.0000e+00 - val_loss: 0.1131 - val_recall: 0.9789 - val_precision: 0.9428 - val_accuracy: 0.0000e+00\nEpoch 343/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1220 - recall: 0.9688 - precision: 0.9287 - accuracy: 0.0000e+00 - val_loss: 0.1076 - val_recall: 0.9789 - val_precision: 0.9411 - val_accuracy: 0.0000e+00\nEpoch 344/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0971 - recall: 0.9844 - precision: 0.9412 - accuracy: 0.0000e+00 - val_loss: 0.1057 - val_recall: 0.9789 - val_precision: 0.9393 - val_accuracy: 0.0000e+00\nEpoch 345/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1006 - recall: 0.9736 - precision: 0.9448 - accuracy: 0.0000e+00 - val_loss: 0.1093 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 346/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1077 - recall: 0.9840 - precision: 0.9309 - accuracy: 0.0000e+00 - val_loss: 0.1094 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 347/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0803 - recall: 0.9917 - precision: 0.9461 - accuracy: 0.0000e+00 - val_loss: 0.1175 - val_recall: 0.9674 - val_precision: 0.9674 - val_accuracy: 0.0000e+00\nEpoch 348/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1092 - recall: 0.9708 - precision: 0.9419 - accuracy: 0.0000e+00 - val_loss: 0.1116 - val_recall: 0.9808 - val_precision: 0.9292 - val_accuracy: 0.0000e+00\nEpoch 349/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0959 - recall: 0.9842 - precision: 0.9425 - accuracy: 0.0000e+00 - val_loss: 0.1066 - val_recall: 0.9789 - val_precision: 0.9445 - val_accuracy: 0.0000e+00\nEpoch 350/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0960 - recall: 0.9841 - precision: 0.9441 - accuracy: 0.0000e+00 - val_loss: 0.1055 - val_recall: 0.9789 - val_precision: 0.9411 - val_accuracy: 0.0000e+00\nEpoch 351/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1049 - recall: 0.9767 - precision: 0.9440 - accuracy: 0.0000e+00 - val_loss: 0.1059 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 352/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1023 - recall: 0.9885 - precision: 0.9358 - accuracy: 0.0000e+00 - val_loss: 0.1047 - val_recall: 0.9789 - val_precision: 0.9481 - val_accuracy: 0.0000e+00\nEpoch 353/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1142 - recall: 0.9677 - precision: 0.9409 - accuracy: 0.0000e+00 - val_loss: 0.1072 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 354/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1078 - recall: 0.9769 - precision: 0.9452 - accuracy: 0.0000e+00 - val_loss: 0.1042 - val_recall: 0.9789 - val_precision: 0.9359 - val_accuracy: 0.0000e+00\nEpoch 355/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1068 - recall: 0.9670 - precision: 0.9502 - accuracy: 0.0000e+00 - val_loss: 0.1032 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 356/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1062 - recall: 0.9800 - precision: 0.9349 - accuracy: 0.0000e+00 - val_loss: 0.1030 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 357/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0947 - recall: 0.9814 - precision: 0.9456 - accuracy: 0.0000e+00 - val_loss: 0.1028 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 358/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0878 - recall: 0.9879 - precision: 0.9431 - accuracy: 0.0000e+00 - val_loss: 0.1024 - val_recall: 0.9751 - val_precision: 0.9568 - val_accuracy: 0.0000e+00\nEpoch 359/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0860 - recall: 0.9788 - precision: 0.9603 - accuracy: 0.0000e+00 - val_loss: 0.1019 - val_recall: 0.9751 - val_precision: 0.9496 - val_accuracy: 0.0000e+00\nEpoch 360/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0877 - recall: 0.9865 - precision: 0.9604 - accuracy: 0.0000e+00 - val_loss: 0.1026 - val_recall: 0.9789 - val_precision: 0.9393 - val_accuracy: 0.0000e+00\nEpoch 361/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1018 - recall: 0.9725 - precision: 0.9442 - accuracy: 0.0000e+00 - val_loss: 0.1026 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 362/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0899 - recall: 0.9868 - precision: 0.9478 - accuracy: 0.0000e+00 - val_loss: 0.1014 - val_recall: 0.9808 - val_precision: 0.9343 - val_accuracy: 0.0000e+00\nEpoch 363/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0952 - recall: 0.9866 - precision: 0.9458 - accuracy: 0.0000e+00 - val_loss: 0.1055 - val_recall: 0.9789 - val_precision: 0.9393 - val_accuracy: 0.0000e+00\nEpoch 364/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0967 - recall: 0.9759 - precision: 0.9354 - accuracy: 0.0000e+00 - val_loss: 0.1015 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 365/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1038 - recall: 0.9832 - precision: 0.9390 - accuracy: 0.0000e+00 - val_loss: 0.0981 - val_recall: 0.9808 - val_precision: 0.9343 - val_accuracy: 0.0000e+00\nEpoch 366/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0982 - recall: 0.9776 - precision: 0.9400 - accuracy: 0.0000e+00 - val_loss: 0.1010 - val_recall: 0.9808 - val_precision: 0.9343 - val_accuracy: 0.0000e+00\nEpoch 367/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1150 - recall: 0.9643 - precision: 0.9387 - accuracy: 0.0000e+00 - val_loss: 0.1011 - val_recall: 0.9789 - val_precision: 0.9428 - val_accuracy: 0.0000e+00\nEpoch 368/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0950 - recall: 0.9743 - precision: 0.9504 - accuracy: 0.0000e+00 - val_loss: 0.1035 - val_recall: 0.9789 - val_precision: 0.9342 - val_accuracy: 0.0000e+00\nEpoch 369/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0832 - recall: 0.9908 - precision: 0.9507 - accuracy: 0.0000e+00 - val_loss: 0.1047 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 370/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0996 - recall: 0.9877 - precision: 0.9404 - accuracy: 0.0000e+00 - val_loss: 0.1068 - val_recall: 0.9789 - val_precision: 0.9342 - val_accuracy: 0.0000e+00\nEpoch 371/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0964 - recall: 0.9830 - precision: 0.9427 - accuracy: 0.0000e+00 - val_loss: 0.1044 - val_recall: 0.9808 - val_precision: 0.9259 - val_accuracy: 0.0000e+00\nEpoch 372/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.1156 - recall: 0.9718 - precision: 0.9327 - accuracy: 0.0000e+00 - val_loss: 0.1048 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 373/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0891 - recall: 0.9830 - precision: 0.9473 - accuracy: 0.0000e+00 - val_loss: 0.1054 - val_recall: 0.9789 - val_precision: 0.9376 - val_accuracy: 0.0000e+00\nEpoch 374/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0839 - recall: 0.9846 - precision: 0.9529 - accuracy: 0.0000e+00 - val_loss: 0.1016 - val_recall: 0.9808 - val_precision: 0.9343 - val_accuracy: 0.0000e+00\nEpoch 375/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0848 - recall: 0.9888 - precision: 0.9579 - accuracy: 0.0000e+00 - val_loss: 0.1038 - val_recall: 0.9770 - val_precision: 0.9427 - val_accuracy: 0.0000e+00\nEpoch 376/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0866 - recall: 0.9783 - precision: 0.9538 - accuracy: 0.0000e+00 - val_loss: 0.1031 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 377/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0900 - recall: 0.9848 - precision: 0.9388 - accuracy: 0.0000e+00 - val_loss: 0.0990 - val_recall: 0.9808 - val_precision: 0.9360 - val_accuracy: 0.0000e+00\nEpoch 378/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0978 - recall: 0.9817 - precision: 0.9389 - accuracy: 0.0000e+00 - val_loss: 0.1010 - val_recall: 0.9808 - val_precision: 0.9343 - val_accuracy: 0.0000e+00\nEpoch 379/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0867 - recall: 0.9791 - precision: 0.9544 - accuracy: 0.0000e+00 - val_loss: 0.1028 - val_recall: 0.9808 - val_precision: 0.9343 - val_accuracy: 0.0000e+00\nEpoch 380/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0870 - recall: 0.9827 - precision: 0.9458 - accuracy: 0.0000e+00 - val_loss: 0.1051 - val_recall: 0.9789 - val_precision: 0.9359 - val_accuracy: 0.0000e+00\nEpoch 381/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0857 - recall: 0.9829 - precision: 0.9547 - accuracy: 0.0000e+00 - val_loss: 0.1052 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 382/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0962 - recall: 0.9822 - precision: 0.9326 - accuracy: 0.0000e+00 - val_loss: 0.1096 - val_recall: 0.9770 - val_precision: 0.9444 - val_accuracy: 0.0000e+00\nEpoch 383/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0941 - recall: 0.9710 - precision: 0.9445 - accuracy: 0.0000e+00 - val_loss: 0.1087 - val_recall: 0.9770 - val_precision: 0.9462 - val_accuracy: 0.0000e+00\nEpoch 384/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0847 - recall: 0.9851 - precision: 0.9535 - accuracy: 0.0000e+00 - val_loss: 0.1115 - val_recall: 0.9789 - val_precision: 0.9359 - val_accuracy: 0.0000e+00\nEpoch 385/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0765 - recall: 0.9861 - precision: 0.9607 - accuracy: 0.0000e+00 - val_loss: 0.1131 - val_recall: 0.9789 - val_precision: 0.9342 - val_accuracy: 0.0000e+00\nEpoch 386/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0867 - recall: 0.9829 - precision: 0.9492 - accuracy: 0.0000e+00 - val_loss: 0.1107 - val_recall: 0.9828 - val_precision: 0.9277 - val_accuracy: 0.0000e+00\nEpoch 387/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1000 - recall: 0.9819 - precision: 0.9417 - accuracy: 0.0000e+00 - val_loss: 0.1141 - val_recall: 0.9789 - val_precision: 0.9342 - val_accuracy: 0.0000e+00\nEpoch 388/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0839 - recall: 0.9832 - precision: 0.9468 - accuracy: 0.0000e+00 - val_loss: 0.1124 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 389/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0877 - recall: 0.9912 - precision: 0.9460 - accuracy: 0.0000e+00 - val_loss: 0.1133 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 390/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0912 - recall: 0.9927 - precision: 0.9398 - accuracy: 0.0000e+00 - val_loss: 0.1141 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 391/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0862 - recall: 0.9907 - precision: 0.9442 - accuracy: 0.0000e+00 - val_loss: 0.1075 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 392/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0902 - recall: 0.9914 - precision: 0.9407 - accuracy: 0.0000e+00 - val_loss: 0.1133 - val_recall: 0.9789 - val_precision: 0.9342 - val_accuracy: 0.0000e+00\nEpoch 393/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.1050 - recall: 0.9857 - precision: 0.9383 - accuracy: 0.0000e+00 - val_loss: 0.1078 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 394/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0793 - recall: 0.9851 - precision: 0.9526 - accuracy: 0.0000e+00 - val_loss: 0.1104 - val_recall: 0.9789 - val_precision: 0.9359 - val_accuracy: 0.0000e+00\nEpoch 395/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0768 - recall: 0.9857 - precision: 0.9540 - accuracy: 0.0000e+00 - val_loss: 0.1144 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 396/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0948 - recall: 0.9876 - precision: 0.9409 - accuracy: 0.0000e+00 - val_loss: 0.1105 - val_recall: 0.9808 - val_precision: 0.9326 - val_accuracy: 0.0000e+00\nEpoch 397/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0893 - recall: 0.9885 - precision: 0.9482 - accuracy: 0.0000e+00 - val_loss: 0.1094 - val_recall: 0.9770 - val_precision: 0.9375 - val_accuracy: 0.0000e+00\nEpoch 398/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0944 - recall: 0.9814 - precision: 0.9493 - accuracy: 0.0000e+00 - val_loss: 0.1072 - val_recall: 0.9808 - val_precision: 0.9275 - val_accuracy: 0.0000e+00\nEpoch 399/400\n43/43 [==============================] - 0s 6ms/step - loss: 0.0913 - recall: 0.9877 - precision: 0.9385 - accuracy: 0.0000e+00 - val_loss: 0.1075 - val_recall: 0.9789 - val_precision: 0.9359 - val_accuracy: 0.0000e+00\nEpoch 400/400\n43/43 [==============================] - 0s 5ms/step - loss: 0.0932 - recall: 0.9834 - precision: 0.9407 - accuracy: 0.0000e+00 - val_loss: 0.1061 - val_recall: 0.9789 - val_precision: 0.9359 - val_accuracy: 0.0000e+00\n","output_type":"stream"}]},{"cell_type":"code","source":"# résumer l'histoire pour précision \nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# résumer l'histoire pour fonction de perte\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:04:45.190217Z","iopub.execute_input":"2021-06-28T09:04:45.190623Z","iopub.status.idle":"2021-06-28T09:04:45.522164Z","shell.execute_reply.started":"2021-06-28T09:04:45.190577Z","shell.execute_reply":"2021-06-28T09:04:45.520758Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"y_pred = keras_model.predict(x_test) # numpy.ndarray\n# print(y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:04:45.523927Z","iopub.execute_input":"2021-06-28T09:04:45.524370Z","iopub.status.idle":"2021-06-28T09:04:45.719853Z","shell.execute_reply.started":"2021-06-28T09:04:45.524321Z","shell.execute_reply":"2021-06-28T09:04:45.718736Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"26/26 [==============================] - 0s 1ms/step\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n  warnings.warn('`model.predict_classes()` is deprecated and '\n","output_type":"stream"}]},{"cell_type":"code","source":"# inverse transformer la prédiction en valeur de chaîne\ny_pred = lb.inverse_transform(y_pred, threshold=None)\n# print(y_pred)\n\n# enregistre dans un fichier CSV\nsavetxt('y_pred_keras.csv', y_pred, delimiter=',', fmt=('%s'))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T09:04:45.721468Z","iopub.execute_input":"2021-06-28T09:04:45.721919Z","iopub.status.idle":"2021-06-28T09:04:45.730950Z","shell.execute_reply.started":"2021-06-28T09:04:45.721871Z","shell.execute_reply":"2021-06-28T09:04:45.729688Z"},"trusted":true},"execution_count":37,"outputs":[]}]}